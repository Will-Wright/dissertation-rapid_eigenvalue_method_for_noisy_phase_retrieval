\chapter{The Improved Gauge Dual Descent Algorithm \\and Performance Results}
\label{Sec:Numerics}


\section{Introduction} 		\label{Subsec:Numerics-intro}

In this chapter we present the Improved Gauge Dual Descent (IGDD) algorithm and examine the performance of the IGDD algorithm.
Section \ref{Subsec:Numerics-improved_PGD} states the IGDD algorithm, which applies the new termination conditions of Chapter \ref{Sec:PLGD_term_crit} and \textit{the IRAM with adaptive parameter selection} of Chapter \ref{Sec:evol_mats} to the GDD algorithm (Algorithm \ref{Alg:PGD}).
Next, Section \ref{Subsec:Numerics-perf_results} examines the performance of the IGDD algorithm.
We demonstrate that the IGDD algorithm is more efficient than the GDD algorithm for a variety of PLGD models with Gaussian noise (\ref{Eqn:PhaseLift-GD_Gaussian_noise}).
We observe that the IGDD algorithm often reduces the number of matrix-vector products in the EMEP (\ref{Eqn:EMEP_PLGD}) by at least $50\%$ as compared with the GDD algorithm.
Further experiments show that this reduction in the number of matrix-vector products is comparable 
to the reduction observed by solving the \emep \ with the empirically optimal (\ref{Def:emp_opt}) number of requested eigenvalues for each EMEP iterate.
Finally, we demonstrate that the Arnoldi decomposition (\ref{Eqn:Arnoldi_decomp}) size $m = 40$ for the IGDD algorithm strikes a proper balance between increasing computational efficiency and minimizing data storage.
Note that all experiments in this chapter involve noisy phase retrieval problems and thus we also use the new termination conditions for the GDD algorithm (otherwise the GDD algorithm would not terminate, as discussed in Section \ref{Subsec:PLGD_term_crit-stagnation}).
All experiments in this chapter are available for reproduction.\footnote{\url{https://github.com/Will-Wright/low-rank-opt-rapid-eig}}






\section{The Improved Gauge Dual Descent Algorithm}	\label{Subsec:Numerics-improved_PGD}


In this section we briefly summarize the contributions of Chapters \ref{Sec:PLGD_term_crit} and \ref{Sec:evol_mats} which lead to the Improved Gauge Dual Descent (IGDD) algorithm.

Section \ref{Subsec:PLGD_term_crit-stagnation} demonstrated that the GDD algorithm fails to converge for PLGD models with Gaussian noise (\ref{Eqn:PhaseLift-GD_Gaussian_noise}).
Section \ref{Subsec:PLGD_term_crit-new_term_crit} then established new termination conditions: the primal difference condition (\ref{Eqn:term_crit_new-primal_difference}) and the dual variable difference condition (\ref{Eqn:term_crit_new-dual_difference}).
Section \ref{Subsec:PLGD_term_crit-new_term_crit} also demonstrated that these new termination conditions accurately identify stagnation of signal recovery progress in the GDD algorithm.
For convenience, we restate these two new termination conditions:
the primal difference condition
\begin{equation}
	\label{Eqn:term_crit_new-primal_difference2}
\frac{| \rho - \hat{\rho} |}{\rho} \leq  \textnormal{tol}_\textnormal{primal} = 10^{-5}
\end{equation}
where $\rho = ||\caA(xx^*) - b||_2$,
and the dual variable difference condition
\begin{equation}
	\label{Eqn:term_crit_new-dual_difference2}
\frac{||y- \hat{y}||_2}{||y||_2} \leq \textnormal{tol}_\textnormal{dual}= 10^{-4},
\end{equation}
where a hat indicates the previous iterate (e.g., $\hat{y} = y_{k-1}$). 


Section \ref{Subsec:evol_mats-adaptive_IRAM} showed that the computational costs of the GDD algorithm are related to the fixed choice of parameters used in the IRAM (Algorithm \ref{Alg:IRAM}), the eigenvalue method used by the GDD algorithm.
The GDD algorithm uses the IRAM with a fixed the number of requested eigenvalues $r = 2$ and Arnoldi decomposition (\ref{Eqn:Arnoldi_decomp}) size $m = 20$.
Section \ref{Subsec:evol_mats-adaptive_IRAM} examined the empirical performance of the IRAM for a range of parameters $r$ to develop \textit{the IRAM with adaptive parameter selection} (Algorithm \ref{Alg:adaptive_IRAM}).
Algorithm \ref{Alg:adaptive_IRAM} increases $m$ to the default value $40$ and adaptively changes $r$ in order to decrease the number of matrix-vector products required by the IRAM.


Applying these the new termination conditions and Algorithm \ref{Alg:adaptive_IRAM} to the GDD algorithm, we see the following steps change in the GDD algorithm (Algorithm \ref{Alg:PGD}).
The input now includes additional parameters ($r_{min}$, $r_{max}$, $m$, $\text{tol}_\text{primal}$, and $\text{tol}_\text{dual}$), and the initialization includes the sets $T = \{ \emptyset \}$ and $R = \{ \emptyset \}$ to track the parameters $t_k$ and $r_k$ for Algorithm \ref{Alg:adaptive_IRAM}.
Step 1 of Algorithm \ref{Alg:PGD} is modified to include the new termination conditions  (\ref{Eqn:term_crit_new-primal_difference2}) and (\ref{Eqn:term_crit_new-dual_difference2}).
Step 2 of Algorithm \ref{Alg:PGD} is modified to perform the eigenvalue computation by passing the necessary parameters to Algorithm \ref{Alg:adaptive_IRAM}.
Finally, step 16 updates the termination parameter $\rho = ||\caA(xx^*) - b||_2$ and the sets $T$ and $R$.
Altogether, these changes result in the Improved Gauge Dual Descent (IGDD) algorithm for optimizing the PLGD model (\ref{Eqn:PhaseLift-GD_Gaussian_noise}).  

\begin{algorithm}[H]
\caption{Improved Gauge Dual Descent (IGDD) algorithm} 	\label{Alg:PGD-improved}

\begin{algorithmic}[1]
	\Statex 	\textbf{Input:} Sensing operator $\caA$ and adjoint $\caA^*$, observation vector $b$,
	initial dual variable $y_0$, 
	estimate of total noise level $\epsilon$,  
	minimum and maximum number of requested eigenvalues, $r_{min}$ and $r_{max}$, 
	Arnoldi decomposition size $m$,
	and convergence tolerances (\ref{Eqn:term_crit_new-primal_difference2}) and (\ref{Eqn:term_crit_new-dual_difference2}).
	\Statex 	\textbf{Output:} Approximate solution signal $x$.
	\Statex		\textbf{Initialization:} Set $T = \{ \emptyset \}$, $R = \{ \emptyset \}$, $k = 0$.
	\While	{conditions (\ref{Eqn:term_crit_new-primal_difference2}) and (\ref{Eqn:term_crit_new-dual_difference2}) are not satisfied}
		\State 		\textit{Compute algebraically largest eigenvalues and corresponding eigenvectors:} 
		Perform Algorithm \ref{Alg:adaptive_IRAM} with inputs $A_k = \caA^*y_k$, $T$, $R$, $r_{min}$, $r_{max}$, and $m$ to obtain	
		$(\lambda_1, v_1)$, $(\lambda_2, v_2)$, $t_k$, $r_k$.
		\State 		\textit{Compute (sub)gradient:} $g = \caA(v_1 v_1^*)$ based on  (\ref{Eqn:GD-subdifferential}).
		\State		Determine differentiability of $\lambda_1(\caA^*y_k)$ based on (\ref{Eqn:GD-diff_tol}).
		\If {$\lambda_1(\caA^*y_k)$ is differentiable}
			\State		\textit{Linesearch:} Perform linesearch (\ref{Eqn:GD-linesearch}) with initial step $\alpha$ (\ref{Eqn:Barzilai-Borwein_steplength}) to obtain $y_{k+1}$.	
		\Else
			\State		\textit{Projected subgradient step:} Set $y_{k+1} = \Pi_\caC (y_k - \alpha g)$ with $\alpha$ from (\ref{Eqn:GD-steplength_sum}).
		\EndIf
		\State		\textit{Primal recovery:} Compute $\hat{x}$ based on (\ref{Eqn:GD-primal_rec2}).
		\State		\textit{Primal refinement:} Find $x_{k+1}$ as the solution to (\ref{Eqn:GD-PFD}) initialized with $\hat{x}$ and $y_{k+1}$.
		\If {$\epsilon = 0$  } {}
			\State		\textit{Dual refinement:} Find $\widehat{y}$ based on (\ref{Eqn:GD-DFP}).
			\State		\textbf{if} $\lambda_1(\caA^* \widehat{y}) < \lambda_1$, set $y_{k+1} = \widehat{y}$.
		\EndIf
			\State	\textit{Update:} Set $\rho_{k+1} = ||\caA(xx^*) - b||_2$. Append $t_k$ to $T$ and $r_k$ to $R$. Set $k = k+1$.
	\EndWhile
	\State	\textit{Return:} $x = x_k$. 
\end{algorithmic}

\end{algorithm}


For our implementation of the IGDD algorithm, we use the default values $r_{min}=2$, $r_{max} = \min\{ 30, m-5 \}$, and $m=40$ (as discussed in Section \ref{Subsec:evol_mats-adaptive_IRAM}).




\section{Performance Results} 		\label{Subsec:Numerics-perf_results}


We now examine the performance behavior of the IGDD algorithm (Algorithm \ref{Alg:PGD-improved}) for a variety of noisy phase retrieval problems.
Section \ref{Subsec:Numerics-perf_results_random} demonstrates that the IGDD algorithm requires $50-80\%$ fewer matrix-vector multiplications than the GDD algorithm for a variety of problems.
Furthermore, Section \ref{Subsec:Numerics-perf_results_emp_optimal} shows that the performance of the IGDD algorithm is comparable to solving the \emep \ with the empirically optimal (\ref{Def:emp_opt}) value $\bar{r}_k$ for each iterate $k$.
We see that the number of requested eigenvalues $r_k$ chosen by the IGDD algorithm effectively tracks the empirically optimal number of requested eigenvalues $\bar{r}_k$, causing the IGDD algorithm to minimize the number of matrix-vector products for a fixed Arnoldi decomposition size $m$.
Finally, Section \ref{Subsec:Numerics-perf_results_emp_arnoldi_param} demonstrates that the default Arnoldi decomposition size $m=40$ for the IGDD algorithm strikes a reasonable balance between improving computational performance and minimizing memory usage.




\subsection{Random Gaussian Signals and Large-Scale Images}	\label{Subsec:Numerics-perf_results_random}

We begin by demonstrating that the IGDD algorithm requires $50-80\%$ fewer matrix-vector multiplications than the GDD algorithm for random signals and large-scale image phase retrieval problems.
Figure \ref{Fig:Numerics-ada_vs_orig_various_params} depicts a set of experiments with randomly generated Gaussian signals.


\begin{figure}[H]
\centering
\hbox{\hspace{-1.1cm} 
	\includegraphics[scale=0.6]{Numerics-ada_vs_orig_various_params}
			}
	\vspace{0.0cm}
	\caption{
	Performance results for PLGD models with Gaussian noise (\ref{Eqn:PhaseLift-GD_Gaussian_noise}), where signals are complex with standard Gaussian distribution (\ref{Def:Gaussian_distribution_complex}).
	Results indicate the GDD algorithm (solid line) and IGDD algorithm (dashed line). 
	Each result is the mean of 10 experiments.
	Left: Varying signal size $n$, with fixed noise ratio $\epsilon_\text{rel} = 0.15$ and oversampling scaled logarithmically with $n$ (i.e., $L = 10, 12, 12, 14$) as indicated in Theorem \ref{Thm:PhaseLift_approx}.
	Middle: Varying oversampling rate $L$, with fixed signal size $n = 128$ and noise ratio $\epsilon_\text{rel} = 0.15$.
	Right: Varying noise ratio $\epsilon_\text{rel}$, with fixed signal size $n = 128$ and oversampling rate $L = 10$.
	}
\label{Fig:Numerics-ada_vs_orig_various_params}
\end{figure}
% experiments.figure.noisyimage_comparison_adaptive_vs_orig

In Figure \ref{Fig:Numerics-ada_vs_orig_various_params} we see that the IGDD algorithm requires fewer matrix-vector products than the GDD algorithm for a wide range of phase retrieval problems varying in size, oversampling rate, and noise ratio.
The left and middle plots in Figure \ref{Fig:Numerics-ada_vs_orig_various_params} suggest that the IGDD algorithm requires about $60\%$ fewer matrix-vector products regardless of signal size $n$ or oversampling rate $L$.
Additionally, the right plot in Figure \ref{Fig:Numerics-ada_vs_orig_various_params} suggests the IGDD algorithm may reduce matrix-vector products by $80\%$ or more for problems with greater noise.
This difference in performance may be related to the evolving spectrum of the \emep \ for noisier phase retrieval problems.
As we saw in Section \ref{Subsubsec:evol_mats-correl_clustering_IRAM_param}, problems with a higher noise ratio (e.g., Figure \ref{Fig:Numerics-surf_mvs_eig_diffs_2}) may see greater clustering of the algebraically largest eigenvalues during early iterations.
Recall that the IRAM (Algorithm \ref{Alg:IRAM}) tends to perform poorly when the number of requested eigenvalues $r$ is such that $\lambda_r$ is clustered with its algebraically neighboring eigenvalues (i.e., $\cdots \approx \lambda_{r+2} \approx \lambda_{r+1} \approx \lambda_r \approx \lambda_{r-1} \approx \lambda_{r-2} \approx \cdots$). 
Since the GDD algorithm calls the IRAM (Algorithm \ref{Alg:IRAM}) with a fixed $r=2$ requested eigenvalues, the GDD algorithm may require far more matrix-vector products than the IGDD for noisier phase retrieval problems.



We also find that the IGDD algorithm is more efficient than the GDD algorithm for image-based phase retrieval problems.
Table \ref{Tab:Numerics-ada_vs_orig_large_images} shows the performance results for two larger images from Figure \ref{Fig:Numerics-large_images}.



\begin{table}[H]
\centering
\begin{tabular}{ |ccc|c|cc|cc| }
 \hline
			&&&  GDD
			&  \multicolumn{2}{c|}{IGDD}
			&	\multicolumn{2}{c|}{IGDD}	\\
Image & $n$ & $L$ &  	& \multicolumn{2}{c|}{$m=40$}  & \multicolumn{2}{c|}{$m=80$}   \\
 \hline
	\multirow{2}{*}{Figure \ref{Fig:Numerics-large_images}, left} &
   \multirow{2}{*}{$57,600$} &  10 &  562,255  &  297,767 & 47\% &  252,316 & 55\% \\ 
  &&  15 &  647,753  &  301,752 & 53\% &  253,209 & 61\% \\ 
	\multirow{2}{*}{Figure \ref{Fig:Numerics-large_images}, right}  &   
     \multirow{2}{*}{$120,000$} &  10 &  852,633  &  364,164 & 57\% &  287,309 & 66\% \\ 
  &&  15 &  563,085  &  291,630 & 48\% &  256,084 & 55\% \\ 
 \hline
\end{tabular}
\vspace{0.5cm}
\caption{
Performance results for PLGD models with Gaussian noise (\ref{Eqn:PhaseLift-GD_Gaussian_noise}), where signals are the images in Figure \ref{Fig:Numerics-large_images}.
Results indicate total number of matrix-vector products and percent decrease from the GDD algorithm.
Parameter $m$ is the Arnoldi decomposition size (\ref{Eqn:Arnoldi_decomp}) for the IRAM (Algorithm \ref{Alg:IRAM}).
	} 
	\label{Tab:Numerics-ada_vs_orig_large_images}
\end{table}
% experiments.figure.noisyimage_comparison_adaptive_vs_orig



\begin{figure}[H]
\centering
	\includegraphics[scale=0.46875]{jul_and_me_orig}
		\hspace{0.1cm}
		\includegraphics[scale=0.5625]{UCD_orig}
		\vspace{0.3cm} \hspace{0.02cm}
		\\
	\includegraphics[scale=0.46875]{jul_and_me_L_15_ada40}	
		\hspace{0.1cm}
		\includegraphics[scale=0.5625]{UCD_L_15_ada40} 
		\vspace{0.0cm}
	\caption{
Images used for experiments in Table \ref{Tab:Numerics-ada_vs_orig_large_images}.
Top left: original image of my daughter and me, image size $240 \times 240 = 57,600$ pixels.
Bottom left: result image after solving EMEP.
Top right: original image of UC Davis roundabout, image size $200 \times 600 = 120,000$ pixels.
Bottom right: result image after solving EMEP.
All experiments have noise ratio $\epsilon_\text{rel} = 0.15$ and oversampling $L = 15$.
	}
\label{Fig:Numerics-large_images}
\end{figure}
% experiments.figure.noisyimage_comparison_adaptive_vs_orig


Table \ref{Tab:Numerics-ada_vs_orig_large_images} demonstrates that the benefits of the IGDD algorithm also apply to large-scale phase retrieval problems.
Additionally, a larger Arnoldi decomposition size $m$ further reduces the number of matrix-vector products.
Thus, when solving large-scale problems it may be advisable to consider increasing this parameter beyond the default setting of $m=40$ in the IGDD algorithm if memory constraints permit this increase.




\subsection{IGDD vs Empirically Optimal Parameter Selection}	\label{Subsec:Numerics-perf_results_emp_optimal}

Next, we demonstrate that the performance improvements of the IGDD algorithm are comparable to the improvements of solving the \emep \ with the empirically optimal (\ref{Def:emp_opt}) value $\bar{r}_k$ for each EMEP iterate $k$.
Table \ref{Tab:Numerics-num_matvecs_opt_vs_ada} indicates the number of matrix-vector products for six PLGD models with Gaussian noise (\ref{Eqn:PhaseLift-GD_Gaussian_noise}) using the GDD algorithm ($r=2$), the IGDD algorithm ($r$ chosen with Algorithm \ref{Alg:adaptive_IRAM}), and the empirically optimal sequence of values $\bar{r}_k$.



\begin{table}[H]
\centering
\begin{tabular}{ |ccc|c|cc|cc| }
 \hline
			&&&  GDD
			&	\multicolumn{2}{c|}{IGDD }
			&  \multicolumn{2}{c|}{ Empirically optimal $\bar{r}$ }	\\
$L$ & $\epsilon_\text{rel}$ & EMEP its &  	& \multicolumn{2}{c|}{ $m=40$ }  & \multicolumn{2}{c|}{$m=40$}   \\
 \hline
 5 &  0.05 & 300 &  406,308  &  198,070 & 51\% &  179,807 & 56\%  \\ 
  5 &  0.15 & 300 & 1,099,045  &  258,385 & 76\% &  242,003 & 78\%  \\ 
  5 &  0.30 &  92 &  444,697  &   69,510 & 84\% &   58,780 & 87\%  \\ 
 10 &  0.05 & 153 &   80,453  &   68,709 & 15\% &   61,948 & 23\%  \\ 
 10 &  0.15 & 108 &   88,317  &   57,231 & 35\% &   51,311 & 42\%  \\ 
 10 &  0.30 &  54 &   72,486  &   25,809 & 64\% &   23,217 & 68\%  \\ 
 \hline
\end{tabular}
\vspace{0.5cm}
\caption{
Performance results for PLGD models with Gaussian noise (\ref{Eqn:PhaseLift-GD_Gaussian_noise}) with original signal from Figure \ref{Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels.
Results indicate total number of matrix-vector products and percent decrease from the GDD algorithm.
Parameter $m$ is the Arnoldi decomposition size (\ref{Eqn:Arnoldi_decomp}).
} \label{Tab:Numerics-num_matvecs_opt_vs_ada}
\end{table}
% experiments.figure.noisyimage_adaptive_eig_full_exp


For each of the experiments in Table \ref{Tab:Numerics-num_matvecs_opt_vs_ada}, we see that the IGDD algorithm offers nearly the same performance improvement as that of solving the EMEP with the empirically optimal values $\bar{r}_k$.
Notably, experiments in Table \ref{Tab:Numerics-num_matvecs_opt_vs_ada} with a low oversampling rate ($L=5$) shows that the IGDD algorithm is particularly effective at decreasing the number of matrix-vector products when there is a large relative difference between the number of matrix-vector products for the GDD algorithm and the empirically optimal choice of values $\bar{r}_k$.
To further explore this performance behavior, Figure \ref{Fig:Numerics-num_eigs_ada_vs_opt} depicts the two PLGD models from Table \ref{Tab:Numerics-num_matvecs_opt_vs_ada} with the largest and smallest relative difference in matrix-vector products (those with $L=5$, $\epsilon_\text{rel} = 0.15$, and $L=10$, $\epsilon_\text{rel} = 0.05$, respectively).




\begin{figure}[H]
\centering
\hbox{\hspace{-1.0cm} \includegraphics[scale=0.6]{Numerics-num_eigs_ada_vs_opt_1_IGDD} }\vspace{0.6cm}
\hbox{\hspace{-1.0cm} \includegraphics[scale=0.6]{Numerics-num_eigs_ada_vs_opt_2_IGDD} }
\vspace{0.2cm}
	\caption{
	Number of requested eigenvalues $r$ in the IRAM (Algorithm \ref{Alg:IRAM}) for two PLGD models from Table \ref{Tab:Numerics-num_matvecs_opt_vs_ada}.
	}
\label{Fig:Numerics-num_eigs_ada_vs_opt}
\end{figure}
% experiments.figure.noisyimage_adaptive_eig_full_exp


In both PLGD models depicted in Figure \ref{Fig:Numerics-num_eigs_ada_vs_opt}, the number of requested eigenvalues $r$ selected by the IGDD algorithm is usually within 1-3 units from the empirically optimal value $\bar{r}$.
The bottom plot in Figure \ref{Fig:Numerics-num_eigs_ada_vs_opt} suggests that the IGDD algorithm required relatively more matrix-vector products than the empirically optimal value $\bar{r}$ because the subroutine Algorithm \ref{Alg:adaptive_IRAM} used in the IGDD algorithm always changes the value of $r$ by one or two units, thus shifting away from the optimal value $\bar{r}=2$ for many EMEP iterates.
As we saw in Section \ref{Subsec:evol_mats-adaptive_IRAM} (e.g., Figure \ref{Fig:Numerics-surf_mvs_eig_diffs_2}) the empirically optimal value $\bar{r}$ may shift rapidly for some PLGD models, and thus Algorithm \ref{Alg:adaptive_IRAM} always changes $r$ by one or two units to continue gathering performance information about the EMEP.





\subsection{Selection of Arnoldi Decomposition Parameter}		\label{Subsec:Numerics-perf_results_emp_arnoldi_param}

Finally, we demonstrate that the default Arnoldi decomposition (\ref{Eqn:Arnoldi_decomp}) size $m=40$ for the IGDD algorithm significantly improves computational performance while limiting memory usage.
Table \ref{Tab:Numerics-num_matvecs_orig_vs_ada} depicts the total number of matrix-vector products required to solve the \emep \ for each PLGD model from Table \ref{Tab:Numerics-num_matvecs_opt_vs_ada} with the IGDD algorithm with various parameter values $m$.

\begin{table}[H]
\centering
\begin{tabular}{ |ccc|c|ccccc| }
 \hline
			  \multicolumn{3}{|c|}{n = 4,096} & GDD 
			&  \multicolumn{5}{c|}{IGDD }	\\
$L$ & $\epsilon_\text{rel}$ & EMEP its & 	& $m=20$  & $m=40$  & $m=60$  & $m=80$  & $m=100$   \\
 \hline
  5 &  0.05 & 300 &  406,308  &  358,195  &  198,070  &  189,401  &  192,042  &  201,270  \\ 
  5 &  0.15 & 300 & 1,099,045  &  806,412  &  258,385  &  224,048  &  214,118  &  215,392  \\ 
  5 &  0.30 &  92 &  444,697  &  175,669  &   69,510  &   56,193  &   55,146  &   54,987  \\ 
 10 &  0.05 & 153 &   80,453  &   77,768  &   68,709  &   64,300  &   68,602  &   73,754  \\ 
 10 &  0.15 & 108 &   88,317  &   65,833  &   57,231  &   53,261  &   54,388  &   55,308  \\ 
 10 &  0.30 &  54 &   72,486  &   28,799  &   25,809  &   24,699  &   25,113  &   25,491  \\ 
 \hline
\end{tabular}
\vspace{0.5cm}
\caption{
Total number of matrix-vector products for various PLGD models with Gaussian noise	(\ref{Eqn:PhaseLift-GD_Gaussian_noise}) with original signal from Figure \ref{Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels.
Parameter $r$ is the number of requested eigenvalues in the IRAM (Algorithm \ref{Alg:IRAM}) and $m$ is the Arnoldi decomposition (\ref{Eqn:Arnoldi_decomp}) size. 
} \label{Tab:Numerics-num_matvecs_orig_vs_ada}
\end{table}
% experiments.figure.noisyimage_adaptive_eig_full_exp



Table \ref{Tab:Numerics-num_matvecs_orig_vs_ada} demonstrates that the IGDD algorithm reduces the number of matrix-vector products from those of the GDD algorithm for all experiments considered.
Yet this cost reduction varies significantly depending on the choice of Arnoldi decomposition (\ref{Eqn:Arnoldi_decomp}) size $m$.
We seek a default setting for the parameter $m$ which is sufficiently large to yield the benefits of the IGDD algorithm, yet sufficiently small as not to impact memory constraints.
To select an appropriate default value for $m$, we examine the two experiments from Table \ref{Tab:Numerics-num_matvecs_orig_vs_ada} with $\epsilon_\text{rel} = 0.15, 0.30$ and $L=5$ which have the greatest original number of matrix-vector products, along with the greatest total decrease in cost when using the IGDD algorithm with a sufficiently large parameter $m$.
Figure \ref{Fig:Numerics-num_matvecs_ada_for_m_vals} singles out these two experiments, depicting the number of matrix-vector products for each \emep \ iteration.

\begin{figure}[H]
\centering
\hbox{\hspace{-0.9cm} \includegraphics[scale=0.6]{Numerics-num_matvecs_ada_for_m_vals} }\vspace{0.0cm}
	\caption{Number of matrix-vector products for each \emep \ iteration from two experiments in Figure \ref{Fig:Numerics-num_matvecs_ada_for_m_vals} with various Arnoldi decomposition size $m=20, 40, 80$.}
\label{Fig:Numerics-num_matvecs_ada_for_m_vals}
\end{figure}
% experiments.figure.noisyimage_adaptive_eig_full_exp




Figure \ref{Fig:Numerics-num_matvecs_ada_for_m_vals} demonstrates that the Arnoldi decomposition size of $m=20$ is not sufficiently large to allow the IGDD algorithm to decrease the number of matrix-vector products.  
The dramatic matrix-vector product spikes for $m=20$ in Figure \ref{Fig:Numerics-num_matvecs_ada_for_m_vals} resemble those first seen in Figure \ref{Fig:EMEP_costs_num_mat_vecs} when using the GDD algorithm.
Yet when the Arnoldi decomposition size is increased to $m=40$, these cost spikes effectively disappear.
The change in number of matrix-vector products between $m=40$ and $m=80$ is minimal for each EMEP iterate.  
Thus the default parameter of $m=40$ for the IGDD algorithm strikes the proper balance between efficiency and memory constraints.













\chapter{Introduction}  \label{Sec:Intro}			\label{Sec:intro}




Phase retrieval has a wide range of solution methods, yet few exist for handling noisy observations without imposing additional restrictions such as signal sparsity.  One recent noisy phase retrieval model which requires no underlying assumptions is the gauge dual of the PhaseLift model (PLGD)
\begin{equation} \label{Eqn:PhaseLift-GD}
\begin{array}{lll}
	&	\min\limits_{\substack{y}}
		&	\lambda_1(\caA^* y)
		\\
\textnormal{(PLGD)}
	&	\st
		&	\langle b, y \rangle - \epsilon ||y|| \geq 1.
\end{array}
\end{equation}
First introduced and analyzed in \cite{DBLP:journals/siamsc/FriedlanderM16}, the PLGD model (\ref{Eqn:PhaseLift-GD}) is based on the PhaseLift method \cite{DBLP:journals/siamis/CandesESV13} in which a desired signal of $n$ elements (e.g., pixels) is lifted into the space of $n \times n$ positive semidefinite matrices, creating a convex, large-scale recovery problem.  
The PLGD model (\ref{Eqn:PhaseLift-GD}) maintains the convergence guarantees of the convex PhaseLift model, yet also allows for efficient first-order methods.


To optimize the PLGD model (\ref{Eqn:PhaseLift-GD}) we use the first-order method proposed in \cite[Section 4.4]{DBLP:journals/siamsc/FriedlanderM16}, which we restate in Section \ref{Subsec:PLGD_algo-algo} as Algorithm \ref{Alg:PGD}.
The authors of \cite{DBLP:journals/siamsc/FriedlanderM16} demonstrate that Algorithm \ref{Alg:PGD} is far more efficient than an alternate method for optimizing the PhaseLift model and returns signals with greater accuracy than wflow \cite{DBLP:journals/tit/CandesLS15}, another method for phase retrieval (see Section \ref{Subsubsec:phase_retrieval-unstructured} for details regarding PhaseLift and wflow).  
Yet the PLGD model (\ref{Eqn:PhaseLift-GD}) faces two significant challenges for noisy phase retrieval.  Computationally, each evaluation of the objective function $\lambda_1(\caA^*y)$ involves a large-scale eigenvalue problem which may require significant runtime for large signals.  Additionally, when the phase retrieval problem has nontrivial noise, first-order methods for the PLGD model (\ref{Eqn:PhaseLift-GD}) such as Algorithm \ref{Alg:PGD} typically fail to converge.


This dissertation offers two main contributions.  
First, we address the convergence challenges of Algorithm \ref{Alg:PGD} for noisy phase retrieval and establish new termination conditions which indicate that signal recovery progress has stagnated.  
Second, we develop a new strategy for handling the sequence of eigenvalue problems in Algorithm \ref{Alg:PGD} which decreases the computational cost and overall runtime of Algorithm \ref{Alg:PGD} by $50-90\%$ for problems with minimal oversampling.


This dissertation is organized in the following manner.  
Chapter \ref{Sec:phase_retrieval} introduces the phase retrieval problem and provides a survey of phase retrieval methods.  
We close Chapter \ref{Sec:phase_retrieval} by demonstrating that Algorithm \ref{Alg:PGD} is generally more accurate for noisy phase retrieval than the wflow method.
Chapter \ref{Sec:PLGD} presents the gauge duality theory necessary for analyzing and optimizing the PLGD model (\ref{Eqn:PhaseLift-GD}).
Chapter \ref{Sec:PLGD_algo} then presents Algorithm \ref{Alg:PGD}, a first-order method for the PLGD model (\ref{Eqn:PhaseLift-GD}), and demonstrates the effectiveness of this algorithm for noiseless phase retrieval.

Chapter \ref{Sec:PLGD_term_crit} demonstrates that Algorithm \ref{Alg:PGD} typically fails to converge for noisy phase retrieval problems. 
We then identify the cause of this behavior and establish new termination conditions for Algorithm \ref{Alg:PGD}.
Chapter \ref{Sec:evol_mats} develops a new, efficient strategy for solving the sequence of eigenvalue problems in Algorithm \ref{Alg:PGD} which we define as the \textit{evolving matrix eigenvalue problem} (EMEP).
We first show that the EMEP is the computational bottleneck of Algorithm \ref{Alg:PGD}.
We see that the spectrum of these eigenvalue problems evolves in a predictable way, with the algebraically largest eigenvalues clustering for later EMEP iterates.
This clustering causes later EMEP iterates to have more difficult eigenvalue problems.
Next, we review the \textit{implicitly restarted Arnoldi method} (IRAM), a common large-scale eigenvalue method and develop an efficient, adaptive strategy (Algorithm \ref{Alg:adaptive_IRAM}) for choosing IRAM parameters to handle the EMEP.
We close Chapter \ref{Sec:evol_mats} by demonstrating that Algorithm \ref{Alg:adaptive_IRAM} effectively tracks the clustering of the algebraically largest eigenvalues from earlier to later EMEP iterates, thus selecting more desirable parameters for the IRAM.


Chapter \ref{Sec:Numerics} provides performance results for Algorithm \ref{Alg:PGD} using the new termination conditions from Chapter \ref{Sec:PLGD_term_crit} and the new eigenvalue strategy (Algorithm \ref{Alg:adaptive_IRAM}) from Chapter \ref{Sec:evol_mats}.
We see that Algorithm \ref{Alg:adaptive_IRAM} decreases the computational cost of the EMEP for a variety of noisy phase retrieval problems.
Chapter \ref{Sec:Conclusion} concludes this dissertation with a summary of our contributions and suggestions for future work.



This dissertation uses the following notation.  Additional notation and definitions related to gauge duality are stated in Sections \ref{Subsec:PLGD-models_intro}.  The $(i,j)$ entry of a matrix $A$ is denoted $[A]_{i,j}$ or $A(i,j)$, and the $i$-th component of a vector $a$ is denoted $a_i$ or $[a]_i$.  Vector norms are the standard $p$-norms, with $||\cdot|| \equiv || \cdot ||_2$.   Matrix norms for $A \in \bbC^{m \times n}$ are Schatten $p$-norms, which apply the  $p$-norm to the vector of singular values, i.e.,
\begin{equation}  \label{Def:shatten_norms}
||A||_p  = \left( \sum_{\substack{i = 1}}^{\substack{\min\{m, n \}}} \sigma_i^p(A) \right)^{1/p}.
\end{equation}
The special case of $p = 2$ gives the Frobenius norm
\begin{equation} 	\label{Def:Frobenius_norm}
||A||_F = \left(   \sum_{\substack{i = 1}}^{\substack{\min\{m, n \}}} \sigma_i^2(A)  \right)^{1/2}.
\end{equation}
Gauge duality is a duality based on multiplicative relations, and thus Schatten norms are essential to gauge duality and developing the PLGD model (\ref{Eqn:PhaseLift-GD}).  In contrast, the EMEP requires measurements with the vector-induced $2$-norm.  Thus we define the \textit{matrix norm} as
\begin{equation} 		\label{Def:matrix_norm}
||A|| = \sup_{\substack{||v||_2 = 1}} ||Av||_2 = \sigma_{\max}(A).
\end{equation}
The standard basis vector is denoted $e_i$, where $[e_i]_i = 1$ and all other components are zero.  
Given a vector $d$ in $\bbR^n$ or $\bbC^n$ with components $d_1, d_2, \ldots, d_n$, the \textit{diagonal operator} is defined as
\begin{equation}
\text{Diag}(d) = \text{Diag}(d_1, d_2, \ldots, d_n)_{ij} = 
	\begin{cases}
		d_i 		&		\text{if } i = j	\\
		0		&	\text{else}.
	\end{cases}
\end{equation}
Additionally, if $A$ is a matrix in $\bbR^{n \times n}$ or $\bbC^{n \times n}$ then the \textit{diagonal operator} is defined as
\begin{equation}
\text{diag}(A) = 
	\begin{bmatrix}
		A(1,1)	\\
		A(2,2)	\\
		\vdots	\\
		A(n,n)
	\end{bmatrix}.
\end{equation}

Given $\caS$, a subset of a finite-dimensional Euclidean space $\caX$, the \textit{indicator} function of $\caS$ is defined as
\begin{equation}  			\label{Def:indicator_function}
\delta_\caS(x) =
	\begin{cases}
		0		&	x \in \caS		\\
		+\infty		&	x \notin \caS.
	\end{cases}
\end{equation}
It is easily seen that if $\caS$ is convex, then $\delta_\caS$ will be convex.  
The indicator function is useful for tasks like embedding a domain constraint of an optimization model into the objective function and is used frequently in Chapter \ref{Sec:PLGD} for proving gauge duality results.

If $\caC$ is a convex subset of a finite-dimensional Euclidean space, then the \textit{normal cone} of $\caC$ at $y_0 \in \caC$ is defined as
\begin{equation} 			\label{Def:normal_cone}
N_\caC(y_0) = \left\{ g \in \caX \ | \ \langle g, y - y_0 \rangle \leq 0 \ \ \forall y \in \caC \right\}.
\end{equation}
By convention, if $y_0$ is not in $\caC$, then $N_\caC(y_0)$ is the empty set.

Given a subspace $S$ of $\bbR^n$ or $\bbC^n$, the \textit{orthogonal complement} of $S$ is defined as
\begin{equation}
S^\perp = \{ v \ | \ \langle v, w \rangle = 0 \ \text{for all} \ w \in S \}.
\end{equation}


Given a symmetric (or Hermitian) matrix $A$ in $\bbR^{n \times n}$ (or $\bbC^{n \times n}$), its eigenvalues are ordered
\begin{equation}			\label{Def:eigenvalues}
\lambda_1(A) \geq \lambda_2(A) \geq \ldots \geq \lambda_n(A),
\end{equation}
where $\lambda_1(A)$ or simply $\lambda_1$ is the algebraically largest eigenvalue of $A$, and $\lambda_n(A)$ or $\lambda_n$ is the smallest eigenvalue.  The \textit{spectrum} of $A$ is the set of all of its eigenvalues $\Lambda = \{ \lambda_1, \lambda_2, \ldots, \lambda_n\}$.
If $S$ is a subspace of $\bbR^n$ (or $\bbC^n$) then $(\theta, u)$ is a \textit{Ritz pair for $A$ with respect to $S$} if 
\begin{equation} 			\label{Def:Ritz_pair_val_vec}
\langle w, (Au-\theta u) \rangle = 0 \hspace{1cm} \forall w \in S.
\end{equation}
Likewise, $\theta$ is a \textit{Ritz value} and $u$ the corresponding \textit{Ritz vector for $A$ with respect to $S$}.

For a pair of matrices $A, B \in \bbC^{n \times n}$, their inner product is induced by the trace
\begin{equation}			\label{Def:trace_inner_product}
\langle A, B \rangle := \tr(A^*B) = \sum_{i=1}^n \sigma_i(A^*B).
\end{equation}
Given $\caC$, a convex subset of a finite-dimensional Euclidean space $\caX$, we define projection of $x \in \caX$ onto $\caC$ as $\Pi_\caC(x)$.  Since the PLGD (\ref{Eqn:PhaseLift-GD}) objective function $f(y) = \lambda_1(\caA^*y)$ is generally nondifferentiable, we consider the subdifferential of $f$.  Given a convex function $f : \caU \rightarrow \bbR$ defined on an open, convex subset $\caU$ of a finite-dimensional Euclidean space $\caX$, the \textit{subdifferential} of $f$ at $y_0$ is defined as
\begin{equation}
	\label{Def:subdifferential}
	\partial f(y_0) = \left\{  g \in \caX \ | \ f(y) \geq f(y_0) + \langle g, y - y_0 \rangle \ \ \forall y \in \caU	\right\},
\end{equation}
and each element of $\partial f(y_0)$ is a \textit{subgradient} of $f$.

Given a linear operator $\caA : \caX \rightarrow \caY$ over finite-dimensional Euclidean spaces $\caX$ and $\caY$, its \textit{adjoint} $\caA^* : \caY \rightarrow \caX$ is defined as the operator which satisfies 
\begin{equation}			\label{Def:adjoint_operator}
\langle \caA x, y \rangle = \langle x, \caA^* y \rangle \hspace{5pt} \text{for all } x \in \caX, y \in \caY. 
\end{equation} 
Since $\caX$ and $\caY$ are finite and $\caA$ is linear, $\caA$ is also continuous.  Thus the Riesz representation theorem guarantees that there will exist a unique linear operator $\caA^*$ \cite[Section 6.2]{reed1980functional}.  In this dissertation, we will be concerned specifically with linear operators $\caA: \caH^n \rightarrow \bbR^m$, where $\caH^n$ is the set of $n \times n$ Hermitian matrices.  It is easily shown that all such linear operators $\caA$ will have the form
\begin{equation}
\caA(X) = \begin{bmatrix}
\langle A_1, X \rangle	\\
\vdots	\\
\langle A_m, X \rangle
\end{bmatrix},
\end{equation}
where each $A_i$ is some matrix in $\caH^n$.  In this case, the adjoint of $\caA$ is given by 
\begin{equation}
\langle \caA(X), y \rangle  	= \sum_{i=1}^m \langle A_i, X \rangle y_i	  = \sum_{i=1}^m \langle y_iA_i, X \rangle   = \langle X, \sum_{i=1}^m  y_iA_i \rangle = \langle X, \caA^*y \rangle,
\end{equation}
where $X \in \caH^n$ and $y \in \bbR^m$.
Thus we have $\caA^*y = \sum_{i=1}^m  y_iA_i$.


The \textit{Gaussian distribution} (or \textit{normal distribution}) $\caN(\mu, \sigma^2)$ is the distribution defined by the probability density function
\begin{equation} 			\label{Def:Gaussian_distribution}
f\left(x \ | \ \mu, \sigma^2\right) = \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}},
\end{equation}
where $\mu$ is the mean and $\sigma^2$ the variance of the distribution.  A real vector has Gaussian distribution $\nu \sim \caN(\mu, \sigma^2)$ if all its elements have Gaussian distribution.  Unless otherwise specified, the Gaussian distribution refers specifically to the \textit{standard Gaussian distribution}, where $\mu = 0$ and $\sigma^2 = 1$.  The \textit{complex standard Gaussian distribution} is defined by the probability density function
\begin{equation} 			\label{Def:Gaussian_distribution_complex}
f(z) = \frac{1}{\pi}e^{-|z|^2}.
\end{equation}



\chapter{Noisy Phase Retrieval} 	\label{Sec:phase_retrieval}




\section{Introduction}  	\label{Subsec:phase_retrieval-intro}


Given the variety of phase retrieval applications and the difficulty of solving the phase retrieval problem (\ref{Eqn:phase_retrieval}), a wide range of phase retrieval methods have been developed for both noisy and noiseless phase retrieval. 
In this chapter we review several of these methods, with particular attention toward methods for noisy phase retrieval.  
These methods are split into three classes: alternating projection methods (Section \ref{Subsubsec:phase_retrieval-alternating_direction_methods}), structured optimization methods (Section \ref{Subsubsec:phase_retrieval-structured}) which rely on additional assumptions like sparsity of the desired signal $\mathbf{x}$, and unstructured optimization methods (Section \ref{Subsubsec:phase_retrieval-unstructured}) which only require an observation vector $b = \mathbf{b} + \eta$ and noise ratio $\epsilon_{\text{rel}} = ||\eta||_2 / ||b||_2$.  
These three classes were chosen to mirror the historical development of these methods (as the alternating projection methods preceded the others by a few decades) and emphasize the uniqueness of the three unstructured optimization methods discussed in Section \ref{Subsubsec:phase_retrieval-unstructured}.
Note that the unstructured methods includes the Gauge Dual Descent (GDD) algorithm, a first-order algorithm which is discussed in Chapter \ref{Sec:PLGD_algo} and central to this dissertation.
Thus Section \ref{Subsubsec:phase_retrieval-unstructured} offers greater detail for theoretical guarantees and numerical performance behavior.



For a recent survey of noiseless phase retrieval methods, see \cite{DBLP:journals/corr/JaganathanEH15a}.
Also note Appendix \ref{Sec:Appx-Comparison} provides an additional comparison of models and algorithms for noisy phase retrieval.
In Appendix \ref{Sec:Appx-Comparison} we show that the PLGD model first discussed in Section \ref{Subsubsec:phase_retrieval-unstructured} is particularly well suited for developing a first-order optimization method, i.e., the GDD algorithm.
Additionally, we demonstrate that the GDD algorithm is generally more accurate and robust to noise and low oversampling than the wflow algorithm also discussed in Section \ref{Subsubsec:phase_retrieval-unstructured}.













\section{Alternating Projection Methods}		\label{Subsubsec:phase_retrieval-alternating_direction_methods}



We begin by discussing alternating projection methods for phase retrieval.  
These methods were established in the 1970s and 1980s as the first strategy for solving (\ref{Eqn:phase_retrieval}) and rely on prior information about the signal, such as support constraints or positivity.  
Originally referred to as \textit{error reduction algorithms}, the algorithms of this section were later identified as part of a larger class of nonconvex alternating projection methods \cite{LeviS84}.  
These methods rely on projecting an iterate between the \textit{object domain} (or time domain) in which we seek the desired signal $\mathbf{x}$, and the \textit{frequency domain} in which we have the observation $b$.  




We begin with two common alternating projection algorithms: the Gershberg-Saxton (GS) algorithm \cite{GS72} and the hybrid input-output (HIO) algorithm \cite{Fienup82}, a variant of the GS algorithm which is still commonly used in practice.  
We then discuss recent alternating projection algorithms for noisy phase retrieval (oversampling smoothness (OSS) \cite{rodriguez2013oversampling}, error reduction (ER-) HIO and noise robust (NR-) HIO \cite{martin2012noise}).
Note that these these alternating projection algorithms are usually not effective for noisy phase retrieval and typically require additional prior information.




\subsection{Gershberg-Saxton Algorithm}

Developed in 1972, the GS algorithm was the first alternating projection algorithm for phase retrieval and serves as an algorithmic foundation which later alternating projection algorithms would modify to improve the likelihood of convergence.  
Given the phase retrieval problem (\ref{Eqn:phase_retrieval}) with no oversampling (i.e., $L=1$) and knowledge of the signal magnitude $c \in \bbR^n_+$ (i.e., $c_i = |\mathbf{x}_i|^2$ for all $i$), we may attempt to solve (\ref{Eqn:phase_retrieval}) with Algorithm \ref{Alg:GS}.

\begin{algorithm}[H]
\caption{Gershberg-Saxton (GS) algorithm}	\label{Alg:GS}

\begin{algorithmic}[1]
	\Statex 	\textbf{Input:} Observation vector $b \in \bbR^n_+$, signal measurement vector $c \in \bbR^n_+$, DFT matrix $F \in \bbC^{n \times n}$.
	\Statex 	\textbf{Output:} Approximate solution signal $x \in \bbC^n$.
	\State 		\textit{Initialize:} Choose a random signal $x_0 \in \bbC^n$, compute DFT $y_0 = F x_0$, set $r_0=|y_0|^2 - b$, $k = 0$.
	\While {$||r_k||_2 > \textnormal{tol}$}
		\State 	Compute DFT of $x_k$ and residual: $y_{k+1} = F x_k$, $r_{k+1} = |y_{k+1}|^2 - b$.
		\State	Impose observation magnitude constraints: $[y_{k+1}]_i = \frac{[y_{k+1}]_i}{|[y_{k+1}]_i|} \sqrt{b_i}$ for all $i = 1, \ldots, n$.
		\State	Compute inverse DFT of $y_{k+1}$: $x_{k+1} = F^{-1} y_{k+1}$.
		\State	Impose signal magnitude constraints: $[x_{k+1}]_i = \frac{[x_{k+1}]_i}{|[x_{k+1}]_i|} \sqrt{c_i}$ for all $i = 1, \ldots, n$.
		\State	$k = k + 1$.
	\EndWhile	\\
	\textit{Return:} $x = x_k$.
\end{algorithmic}
\end{algorithm}

During an iteration of the GS algorithm, knowledge of the signal and observation magnitudes, as well as any additional information, is applied when the iterate reaches that respective domain (steps 6 and 4, respectively).  
While the GS algorithm is notable as the first phase retrieval algorithm, this algorithm is also very likely to converge to a local rather than global minimum \cite{DBLP:journals/corr/JaganathanEH15a}.



\subsection{Hybrid Input-Output Algorithm}

In \cite{Fienup82},  Fienup interpreted the GS algorithm as a nonlinear feedback control system (Figure \ref{Fig:nonlinear_feedback_control_system}), where the \textit{System} component corresponds to the map $\widetilde{P}_\textnormal{freq} = F^{-1} P_\textnormal{freq} F$ (combining steps 3-5 of the GS algorithm, where $P_\textnormal{freq}$ is projection onto the frequency constraints in step 4).  
Since the GS algorithm interprets phase retrieval as an error-reduction problem, the system output $\widetilde{P}_\textnormal{freq}(x_k)$ is viewed as the current candidate for the desired signal $\mathbf{x}$, and the signal magnitude constraints (GS algorithm, step 6) are imposed to arrive at a new input $x_{k+1}$ which is considered the current approximation to the solution signal.  
By viewing phase retrieval as a nonlinear feedback problem, the input $x_k$ is no longer treated as a signal approximation, and instead serves as feedback information for the system.  Thus $x_k$ need not satisfy the signal magnitude constraints.


\begin{figure}[H]
\centering
\tikzstyle{block} = [rectangle, draw, fill=white!20,
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']

\begin{tikzpicture}[align=center, node distance = 5cm, auto]
    % Place nodes
    \node [block] (system) {System};
    \node [block, right of=system] (sensor) {Sensor};
	\node [block, left of=system] (controller) {Controller};
	\draw[<-] (controller) -- +(-25mm,0) node [left] (reference) {} node [below] {reference};
	\draw[->] (sensor) -- +(25mm,0) node [right] (solution) {} node [below] {solution};

	% Draw edges
	\path [->] (system) edge node [below] {output}
										    node [above] {$\widetilde{P}_\textnormal{freq}(x_k)$} (sensor);
	\path [line] (sensor) -- ++  (0,-3cm) -| (controller);
	\path [->] (controller) edge node [below] {input} node [above] {$x_k$} (system);
\end{tikzpicture}
\caption{Phase retrieval as a nonlinear feedback control system} 
\label{Fig:nonlinear_feedback_control_system}
\end{figure}


Fienup observed that a small change in the input will result in an output that is approximately a constant $\alpha$ times the change in the input, that is
\begin{equation} 		\label{Eqn:HIO_linear_appx}
	\widetilde{P}_\textnormal{freq}(x + \Delta x) - \widetilde{P}_\textnormal{freq}(x) \approx \alpha \Delta x.
\end{equation}
To force a change of $\Delta x$ in the output $\widetilde{P}_\textnormal{freq}(x)$, the input would logically be changed by $\beta \Delta x$, where $\beta = 1/\alpha$ from (\ref{Eqn:HIO_linear_appx}).  
This observation led Fienup to identify three new potential strategies for selecting an update (the \textit{Controller} in Figure \ref{Fig:nonlinear_feedback_control_system}).  
The most successful of these strategies involves replacing step 6 in the GS algorithm with the index-wise update
\begin{equation} 		\label{Eqn:HIO_step_6}
[x_{k+1}]_i =
	\begin{cases}
		[\widetilde{P}_\textnormal{freq}(x_k)]_i	&	i \notin \caV		\\
		[x_k]_i - \beta [\widetilde{P}_\textnormal{freq}(x_k)]_i			&	 i \in \caV,
	\end{cases}
\end{equation}
where $\caV$ is the set of indices in which the update violates the object domain constraints.
If we replace step 6 of the GS algorithm with (\ref{Eqn:HIO_step_6}), then we have the HIO algorithm.
This simple corrective step (\ref{Eqn:HIO_step_6}) which HIO applies to the object domain makes the HIO algorithm much more likely than the GS algorithm to recover a signal from a low-noise observation \cite{DBLP:journals/corr/JaganathanEH15a}.   
Nevertheless, the HIO algorithm still has a tendency to converge to local minima and is not robust to noise.  
In practice, the user will often select a large number of random initial iterates to initialize the HIO algorithm and select the best resulting signal.





\subsection{HIO-Type Methods}


Recent developments in alternating projection algorithms for phase retrieval have focused on handling noisy observations.  In \cite{rodriguez2013oversampling}, the authors modify the GS algorithm by taking the HIO update (\ref{Eqn:HIO_step_6}) and performing a Gaussian smoothing step in the frequency domain on the indices (pixels) violating the support constraint.  
Their oversampling smoothness (OSS) algorithm takes the update $x_{k+1}$ from step 6 of the HIO algorithm and applies the additional update
\begin{equation}
[x_{k+1}]_i =
	\begin{cases}
		[x_{k+1}]_i		&	i \notin \caV		\\
		\left[ F^{ -1} \left[ F ( x_{k+1}) \ .* \mathbf{w}(j, \alpha) \right]  \right]_i	&	 i \in \caV,
	\end{cases}
\end{equation}
where $j = 1, \ldots, N$ is the Fourier index, $.*$ is pointwise multiplication, and $\mathbf{w}(j, \alpha)$ is a normalized Gaussian function
\[
\mathbf{w}(j, \alpha) = e^ {- \frac{1}{2} \left(\frac{j}{\alpha} \right)^2}.
\]
As $\alpha \rightarrow \infty$, the original HIO algorithm is recovered.  
The authors select $\alpha$ heuristically, with early $\alpha = \caO(N)$ and later $\alpha = \caO(1/N)$, causing the OSS algorithm to behave similarly to the HIO algorithm during early iterations while damping high frequency violations in later iterations.  
Their experiments \cite[Section 3]{rodriguez2013oversampling} apply Poisson noise with noise levels ranging from 0.05 to 0.25 (and measured in the $1$-norm, but approximately equivalent to our definition $\epsilon_\textnormal{rel} = ||\eta||_2 / ||b||_2$).  
Their results suggest that the accuracy and consistency of the OSS algorithm is superior to HIO and two HIO variants: ER-HIO and NR-HIO of \cite{martin2012noise}.  



While HIO-type methods are computationally efficient and still commonly used in practice \cite{DBLP:journals/corr/JaganathanEH15a}, they also require special parameter tuning (e.g., $\beta$ in HIO and $\alpha$, $\beta$ in OSS) and prior information about the desired signal.  
Additionally, none of these methods are wholly robust to noise or guaranteed to converge, and require large batches of random initializations in practice to generate an adequate solution.  
To overcome these deficiencies, recent phase retrieval methods have largely avoided the alternating projection framework, instead casting (\ref{Eqn:phase_retrieval}) as a structured optimization problem.





\section{Structured Optimization Methods}  	\label{Subsubsec:phase_retrieval-structured}


Next we discuss structured optimization methods for noisy phase retrieval.  In the past decade, a wide range of methods have been crafted to take advantage of the structure of particular phase retrieval problems.  The survey in this section highlights typical methods and summarizes a few theoretical guarantees for exact recovery or error bounds.  
The methods in this section are grouped in terms of the structural property each method seeks to exploit.  
The \textit{compressive phase retrieval} methods assume sparsity in the desired signal $\mathbf{x}$, and include \cite{DBLP:journals/corr/abs-1104-4406}, the GrEedy Sparse PhAse Retrieval (GESPAR) method \cite{shechtman2014gespar}, and a thresholded wflow algorithm \cite{cai2016optimal}.
The \textit{robust phase retrieval} methods assume sparsity in either the observation $b$ \cite{katkovnik2017phase} or the noise $\eta$ \cite{jiang2017robust}.
The \textit{supervised phase retrieval} methods require an approximate solution signal $\hat{x}$ for initialization, and include \cite{goldstein2018phasemax} and \cite{bahmani2016phase}.




\subsection{Compressive Phase Retrieval Methods}

We begin by summarizing a few recent compressive phase retrieval methods and theoretical results.
The authors of \cite{DBLP:journals/corr/abs-1211-0872} consider the theoretical recovery guarantees for compressive phase retrieval.
They prove signal uniqueness and stable recovery (a stronger property than invertibility) for a variety of sparsity assumptions.  
For instance, if the signal $\mathbf{x}$ is $k$-sparse and observation $b$ is noiseless, then $\caO(k \log(n/k))$ observations are necessary for signal uniqueness.  
If $\mathbf{x}$ is $k$-sparse and $b$ is noisy, then $\caO(k \log(n/k) \log(k))$ observations are necessary to guarantee stable recovery (which gives $\caO(n \log(n))$ observations if $\mathbf{x}$ dense).


Many methods have been considered for handling compressive phase retrieval.  
In \cite{DBLP:journals/corr/abs-1104-4406}, the authors consider sparse signals with noisy observations and proceed by lifting the signal and sensing vectors (for details on lifting and the PhaseLift model, see Section \ref{Subsubsec:phase_retrieval-unstructured}).  
They construct a rank minimization problem similar to the PhaseLift rank minimization model (see Section \ref{Subsubsec:phase_retrieval-unstructured}), but with an additional mixed $1,2$-norm constraint
\[
	\sum\limits_{\substack{i = 1}}^{\substack{n}}
	\left( 	\sum\limits_{\substack{j = 1}}^{\substack{n}}  X_{i, j}^2
	\right)^{1/2}
	\leq \zeta
\]
which promotes row-sparsity of the lifted solution matrix $X$.  The resulting algorithm also includes a thresholding step on the spectrum of $X$ to enforce low-rankness in the solution.   
The authors provide comparative numerical results indicating the addition of this constraint/thresholding strategy in their optimization method decreases reconstruction error as the noise ratio increases.  
However, the overall model is nonconvex, the iterations are expensive (each requiring the inversion of a lifted matrix), and no theory is provided to guarantee convergence or signal recovery quality.


The GESPAR method of \cite{shechtman2014gespar} assumes $\mathbf{x}$ is $k$-sparse and $b$ is noisy, and constructs an algorithm which maintains an active set $S$ of indices which converges to the appropriate $k$-sized set of active indices in the solution signal.  
This local search method does not require matrix lifting, making it efficient for large-scale phase retrieval.  
Numerical results \cite[Section 5]{shechtman2014gespar} indicate that as sparsity increases, GESPAR has a higher recovery probability than a PhaseLift algorithm and a sparse variant of the HIO algorithm.



A thresholded version of the wflow algorithm (see Section \ref{Subsubsec:phase_retrieval-unstructured}) is considered in \cite{cai2016optimal}.
Here, the authors assume that $\mathbf{x}$ is sparse and $b$ is noisy, and add the penalty $\tau ||x||_1$ to the wflow objective function.
The resulting wflow-type model
\begin{equation}
\begin{array}{ll}
	\min\limits_{\substack{x}}
		&	\frac{1}{2m} \sum\limits_{\substack{i=1}}^{\substack{m}} \left( |a_i^*x|^2 - b_i \right)^2
			+ \tau ||x||_1,
\end{array}
\end{equation}
is a phase retrieval analog to the basis pursuit denoising model \cite{chen2001atomic}.
The authors show that for the proper choice of $\tau$, their thresholded wflow algorithm has the minimax optimal rate of convergence, $\caO (k \log(n) / m)$.


Another recent method for compressive phase retrieval considers unstructured noise $\eta$ in the observation, and interprets the recovery of a $k$-sparse signal $\mathbf{x}$ as a covariance maximization problem between the observations $b_i$ and the sensing values $|a_i^*x|^2$ over the appropriate $k$-dimensional subspace \cite{zhang2017fast}.  
The authors show that only $\caO(k)$ measurements are required for their algorithm to converge within $\epsilon$ error in a runtime of $\caO(nk \log(1/ \epsilon))$ iterations.




\subsection{Robust Phase Retrieval Methods}


Next we consider robust phase retrieval methods, where we find sparsity in the observation $b$ rather than the signal $\mathbf{x}$.  
The authors of \cite{jiang2017robust} assume the noise $\eta$ is sparse.  
Whereas the minimization of the $2$-norm is optimal for fitting against Gaussian noise, the $1$-norm is more robust for distributions with heavier tails.  
Thus the authors minimize the objective function $\sum\limits_{\substack{i=1}}^{\substack{m}} \left| |a_i^*x|^2 - b_i \right|$ using an alternating direction method of multipliers (ADMM) algorithm.  
Their numerical results demonstrate that their algorithm achieves better accuracy than the wflow algorithm when recovering from an observation with Gaussian noise and 10\% outliers.

In contrast, the authors of \cite{katkovnik2017phase} consider phase retrieval when the observation vector $b$ is itself sparse and noisy.  
They develop an HIO-type algorithm with noise suppression in the frequency domain and phase/amplitude filtering in the object domain.  
Numerical results show this algorithm achieves better accuracy under the given sparsity assumptions than the wflow algorithm and a filtered version of the GS algorithm.




\subsection{Supervised Phase Retrieval Methods}

The final set of structured optimization methods we consider are the supervised phase retrieval methods.
Rather than assuming sparsity of the signal or observations, the authors of \cite{goldstein2018phasemax} and \cite{bahmani2016phase} assume the existence of an approximation $\hat{x}$ to the desired signal $\mathbf{x}$.  In this supervised phase retrieval paradigm, the authors define the PhaseMax model
\begin{equation} 			\label{Eqn:PhaseMax}
\begin{array}{lll}
	\max\limits_x	&	\Re \langle \hat{x}, x \rangle 		\\
	\st 			&	| \langle a_i, x \rangle |^2 \leq b_i, 	&	i = 1, \ldots, m,
\end{array}
\end{equation} 
where $\Re(\cdot)$ maps a complex number to its real component.  The PhaseMax model (\ref{Eqn:PhaseMax}) has the benefits of being convex without requiring signal lifting.  The dual to this problem is the widely-studied basis pursuit problem which has several efficient solution techniques \cite{chen2001atomic}, \cite{candes2006stable}.  The authors of \cite{goldstein2018phasemax} prove that the probability of exact recovery is dependent on the angle between $\mathbf{x}$ and $\hat{x}$.
Their numerical results demonstrate that this probability quickly approaches $1$ as oversampling increases.





\section{Unstructured Optimization Methods}	 	\label{Subsubsec:phase_retrieval-unstructured}


The final class of methods we examine are unstructured optimization methods.  Unlike the methods discussed in Section \ref{Subsubsec:phase_retrieval-structured}, these methods place no assumptions on the signal $\mathbf{x}$, the sensing vectors $a_i$, or the observation $b = \mathbf{b} + \eta$ other than knowledge of the noise ratio $\epsilon_\rel = ||\eta||_2 / ||b||_2$.

This section begins with an explanation of matrix lifting for the signal and sensing vectors.  
Next we discuss the PhaseLift model \cite{DBLP:journals/siamis/CandesESV13} and the wflow algorithm \cite{DBLP:journals/tit/CandesLS15}.   
Theoretical and numerical results are included to highlight the effectiveness and robustness of these models and methods (for complete theoretical results, see \cite{DBLP:journals/tit/CandesLS15},  \cite{sun2016geometric} for wflow and \cite{candes2014solving}, \cite{candes2013phaselift} for PhaseLift).  
This discussion of PhaseLift and wflow leads us to the PhaseLift gauge dual model \cite{DBLP:journals/siamsc/FriedlanderM16}, another unstructured optimization model which is the subject of Chapters \ref{Sec:PLGD} and \ref{Sec:PLGD_algo}.


%\vspace{-1.9cm}


\subsection{PhaseLift}

The PhaseLift model was first introduced in \cite{DBLP:journals/siamis/CandesESV13}, with additional theoretical results established in \cite{candes2013phaselift}.  This model is based on the concept of matrix lifting.  First we define the linear operator $\caA$ which allows us to lift the nonlinear phase retrieval observation constraint from (\ref{Eqn:phase_retrieval}) into a higher-dimensional linear constraint.  If we lift the $n$-dimensional signal $x$ and sensing vectors $a_i$ into rank-one Hermitian matrices $X = xx^*, A_i = a_ia_i^* \in \caH^n$, then the sensing operator $\caA$ is defined coordinate-wise to satisfy $[\caA (X)]_i = \langle A_i, X \rangle = \tr( a_i a_i^* xx^* )= | \langle a_i, x \rangle |^2$.  Thus $\caA: \caH^n \rightarrow \bbR^m$ and its adjoint $\caA^* : \bbR^m \rightarrow \caH^n$ are defined as
\begin{equation} 			\label{Eqn:A_definition}
\begin{array}{lcr}
\caA(xx^*)
	:= \begin{bmatrix} \langle a_1 a_1^*, xx^* \rangle \\ \vdots \\ \langle a_m a_m^*, xx^* \rangle \end{bmatrix}
		
	& 	\text{and}
		&	\caA^*y
			  := \sum\limits_{\substack{i = 1}}^{\substack{m}} y_ia_ia_i^*.
\end{array}
\end{equation}
In particular, if the experimental model is the one discussed in Section \ref{Subsec:phase_retrieval-applications} then we have
\begin{equation} 			\label{Eqn:A_definition_with_masks}
\begin{array}{l}
\caA(xx^*)
		= \begin{vmatrix}
				FC_1x \\ \vdots \\ FC_Lx
			\end{vmatrix}^2
			= \textnormal{diag}\begin{bmatrix}
			\begin{pmatrix}
			FC_1 \\ \vdots \\ FC_L
			\end{pmatrix}
		 	(xx^*)
	 		\begin{pmatrix}
			FC_1 \\ \vdots \\ FC_L
		 	\end{pmatrix}^*
		\end{bmatrix},
									\\
									\\
\caA^*y
			  =\sum\limits_{\substack{j=1}}^{\substack{L}}
					[FC_j]^* \textnormal{Diag}(y_j) F C_j.
\end{array}
\end{equation}






The lifted rank-one matrix $X = xx^* \in \caH^n$ and sensing operator $\caA$ are then used to lift the nonlinear phase retrieval constraints $| \langle a_i, x \rangle | ^2 = b_i$ for $i = 1, \ldots, m$ into the linear constraint $\caA(X) = b$.  Thus (\ref{Eqn:phase_retrieval}) is equivalent to the left-most problem in the sequence of problems
\begin{equation} 		\label{Eqn:PhaseLift_rank_model}
\begin{array}{ll}
		\textnormal{find}
		&	x
			\\
		\st
		& 	\caA (xx^*) = b
\end{array}
\iff
\begin{array}{ll}
		\textnormal{find}
		&	X
			\\
		\st
		& 	\caA (X) = b
			\\
		&	X \succeq 0
			\\
		&	\textnormal{rank}(X) = 1
\end{array}
\iff
\begin{array}{ll}
		\min\limits_{\substack{X \in \caH^n}}
		&	\textnormal{rank}(X)
			\\
		\st
		& 	\caA (X) = b
			\\
		&	X \succeq 0.
\end{array}
\end{equation}
If a rank-one solution  $X = xx^*$ exists then this matrix satisfies the lifted model in the middle of (\ref{Eqn:PhaseLift_rank_model}), and the left implication holds.  Likewise, the rank minimization model at right in (\ref{Eqn:PhaseLift_rank_model}) will have a rank-one solution, and the right implication holds.  





The resulting rank minimization problem at right of (\ref{Eqn:PhaseLift_rank_model}) is NP hard, as it includes the cardinality minimization problem as a special case (see \cite{natarajan1995sparse} or  \cite{recht2010guaranteed}).  Thus the next step is to relax the discrete, nonconvex objective function $\textnormal{rank}(X)$.  To generalize the right-most model in (\ref{Eqn:PhaseLift_rank_model}) for noisy phase retrieval, a norm bound is also applied to the sensing constraint.  This gives the semidefinite program which defines the PhaseLift model \cite{DBLP:journals/siamis/CandesESV13}, \cite{candes2013phaselift}
\begin{equation} \label{Eqn:PhaseLift}
\begin{array}{lll}
	&	\min\limits_{\substack{X \in \caH^n}}
		&	||X||_1 = \sum\limits_{\substack{i=1}}^{\substack{n}} \sigma_i(X)
			\\
\textnormal{(PhaseLift)}
	&	\st
		& 	||\caA (X) - b||_2 \leq \epsilon
			\\

	&
		&	X \succeq 0.

\end{array}
\end{equation}
Here the objective function $||X||_1$ refers to the Schatten $p$-norm (which is expressed as $\sum_{i=1}^n \sigma_i(X)$, $\tr(X)$, or $\langle I, X \rangle$ in various literature).
While we can evaluate the function $||X||_1$ by computing the trace of $X$, evaluating the subdifferential of this function requires a (partial) SVD.
Also note that the term $\epsilon = ||\eta||_2$ measures the total noise, whereas $\epsilon_\rel = ||\eta||_2 / ||b||_2$ discussed in Section \ref{Subsec:phase_retrieval-applications} measures the noise ratio (or relative noise).  We choose the Schatten $p$-norm to highlight the fact that (\ref{Eqn:PhaseLift}) is the semidefinite analog to the celebrated  basis pursuit denoising problem \cite{chen2001atomic}, \cite{candes2006stable},

\begin{equation}  			\label{Eqn:basis_pursuit}
\begin{array}{lll}
	&	\min\limits_{\substack{x}}
		&	||x||_1
			\\
	&	\st
		&	||Ax-b||_2 \leq \epsilon,
\end{array}
\end{equation}
where minimizing $||x||_1$ serves as a convex relaxation to minimizing the discrete, nonconvex function $||x||_0 = \textnormal{nnz}(x)$.



The next two theorems quantify the relaxation of the PhaseLift model (\ref{Eqn:PhaseLift}) from the phase retrieval model (\ref{Eqn:phase_retrieval}), establishing exact and approximate recovery guarantees for the PhaseLift model \cite{candes2014solving}, \cite{candes2013phaselift}.  Theorem \ref{Thm:PhaseLift_exact} applies to the noiseless case (where $\epsilon = 0$ in the PhaseLift model), establishing a probability of equivalence between (\ref{Eqn:phase_retrieval}) and (\ref{Eqn:PhaseLift}).  


\begin{theorem}  \label{Thm:PhaseLift_exact}
Consider an arbitrary signal $\mathbf{x}$ in $\bbR^n$ or $\bbC^n$ and assume the sensing vectors $a_i$ are independent, uniformly distributed on the unit sphere of $\bbR^n$ or $\bbC^n$.  Suppose that the number of measurements obeys $m \geq c_0 n$, where $c_0$ is a sufficiently large constant. Then in both the real and complex cases, the solution to (\ref{Eqn:PhaseLift}) is exact with high probability in the sense that the noiseless PhaseLift problem (i.e., (\ref{Eqn:PhaseLift}) with $\epsilon = 0$) has a unique solution obeying
\[
X = \mathbf{x}\mathbf{x}^*.
\]
This holds with probability at least $1 - \caO(e^{-\gamma m})$, where $\gamma$ is a positive absolute constant.
\end{theorem}
\begin{proof}
See \cite[Section 2]{candes2014solving}.
\end{proof}




The assumptions of Theorem \ref{Thm:PhaseLift_exact} are met by the experimental models used in \cite{DBLP:journals/siamis/CandesESV13}, \cite{candes2013phaselift}, \cite{DBLP:journals/siamsc/FriedlanderM16}, and this dissertation (see Section \ref{Subsec:phase_retrieval-applications}, where the masks $C_i \in \bbC^{n \times n}$ from (\ref{Eqn:FCx}) have diagonal entries chosen with Gaussian distribution).  Thus, if $L$ is the oversampling rate (i.e., $m = nL$) then Theorem \ref{Thm:PhaseLift_exact} shows that only $L = \caO(1)$ masks are required to guarantee exact signal recover (up to global phase) with high probability.




Theorem \ref{Thm:PhaseLift_approx} applies to the  noisy phase retrieval case ($\epsilon > 0$), where there is no guarantee that the PhaseLift (\ref{Eqn:PhaseLift}) solution matrix $X$ is low rank.  Thus the solution signal $x$ is set to an appropriate rescaling of the eigenvector $v_1$ corresponding to the algebraically largest eigenvalue $\lambda_1 = \lambda_1 (X)$, giving
\begin{equation}  			\label{Eqn:PhaseLift_solution_signal}
x = \sqrt{\lambda_1}v_1.
\end{equation}

\begin{theorem}  \label{Thm:PhaseLift_approx}
Consider an arbitrary signal $\mathbf{x}$ in $\bbR^n$ or $\bbC^n$ and assume the sensing vectors $a_i$ are independent, uniformly distributed on the unit sphere of $\bbR^n$ or $\bbC^n$.  Also suppose that the number of measurements obeys $m \geq C_0 n \log n $, where $C_0$ is a sufficiently large constant.  Then the PhaseLift (\ref{Eqn:PhaseLift}) solution $X$ obeys
\begin{equation}				\label{Eqn:Thm_PhaseLift_approx_X}
||X - \mathbf{x}\mathbf{x}^*||_F \leq C_0 \epsilon,
\end{equation}
for some positive constant $C_0$.  Additionally, we have
\begin{equation}		\label{Eqn:Thm_PhaseLift_approx_x}
||x - e^{i \theta}\mathbf{x}||_2 \leq C_0 \min(||\mathbf{x}||_2, \epsilon / ||\mathbf{x}||_2),
\end{equation}
where $x$ is defined as in (\ref{Eqn:PhaseLift_solution_signal}) and $\theta \in [0, 2\pi]$.
Both of these estimates hold with probability at least $1 - \caO(e^{-\gamma \frac{m}{n}})$, where $\gamma$ is a positive absolute constant.
\end{theorem}
\begin{proof}
See \cite[Section 6]{candes2013phaselift}.
\end{proof}


The strength of the PhaseLift model lies in its convexity and generality (no signal or observation assumptions are necessary).  
However, in practice the objective function $||X||_1$ is difficult to optimize, as evaluation of the subdifferential $\partial ||X||_1$ requires a (partial) SVD of $X$.





\subsection{Wflow}


Like the PhaseLift model, the recent Wirtinger flow (wflow) model \cite{DBLP:journals/tit/CandesLS15} also seeks to solve the phase retrieval problem (\ref{Eqn:phase_retrieval}) without any assumptions on the structure or sparsity of the signal or observation.  However, unlike the PhaseLift model, the wflow model does not involve matrix lifting and instead frames (\ref{Eqn:phase_retrieval}) as the least-squares problem
\begin{equation} 				\label{Eqn:wflow_least-squares}
\begin{array}{ll}
	\min\limits_{\substack{x}}
		&	\frac{1}{2m} \sum\limits_{\substack{i=1}}^{\substack{m}} \left( |a_i^*x|^2 - b_i \right)^2
			= \frac{1}{2m} || \caA(xx^*) - b||_2^2.
\end{array}
\end{equation}


While the wflow model (\ref{Eqn:wflow_least-squares}) is nonconvex, the authors of \cite{DBLP:journals/tit/CandesLS15} develop an efficient gradient descent-like algorithm (the wflow algorithm) which has a provable guarantee for exact recovery when initialized appropriately.
Initialization of the wflow algorithm involves a rescaling of the eigenvector $v_1$ corresponding to the algebraically largest eigenvalue of the matrix $\caA^*b$, which is itself the sum of the outer products of the measurement vectors, scaled with the observation magnitudes
\begin{equation}			\label{Eqn:wflow_initialization}
\caA^*b := \sum\limits_{\substack{i=1}}^{\substack{m}}b_i a_i a_i^*.
\end{equation}


The authors motivate this choice of initialization by noting that if the sensing vectors $a_i$ are i.i.d. with standard Gaussian distribution, and $||\mathbf{x}||_2 = 1$, then the expected value of the outer product sum $\caA^*b$ will be
\begin{equation} 		\label{Eqn:wflow_expected_value}
\bbE \left[ \frac{1}{m}\caA^* b \right] = I + 2\mathbf{x}\mathbf{x}^*.
\end{equation}
Thus for large $m$, (\ref{Eqn:wflow_initialization}) has a high probability of returning a vector $v_1$ closely aligned with the solution $\mathbf{x}$. Intuitively, this initialization can also be seen as maximizing $\langle \caA(vv^*), b \rangle	= \langle v v^*, \caA^* b \rangle = \langle v, [\caA^* b] v \rangle$.  Hence selecting the eigenvector $v_1$ corresponding to the algebraically largest eigenvalue of $\caA^*b$ should promote $\caA(v_1v_1^*)$ being collinear with $b$.



With this initialization, the authors recommend a gradient descent-like method with a preset stepsize strategy.  Let $f(x) = \frac{1}{2m}||\caA(xx^*) - b||_2^2$, and the residual $r = \frac{1}{m}[\caA(xx^*) - b]$.  The mapping $f: x \rightarrow \frac{1}{2m}||\caA(xx^*) - b||_2^2$ from $\bbC^n$ to $\bbR$ is not holomorphic, and thus not complex-differentiable.  As a result, the authors appeal to Wirtinger derivatives \cite[Section 6]{DBLP:journals/tit/CandesLS15} for the descent direction
\begin{equation}			\label{Eqn:wflow_derivative}
\begin{split}
\nabla f(x)
	&	= \frac{1}{m} [\caA^*(\caA(xx^*) - b)]x
		\\
	&	= \frac{1}{m}  [\caA^*r]x
		\\
	& =  \frac{1}{m}  \left[ \sum\limits_{\substack{j=1}}^{\substack{L}}
					C_j^* F^* \textnormal{Diag}(r_j) F C_j \right] x.
\end{split}
\end{equation}
The stepsize is then chosen using the heuristics \cite[Section 2]{DBLP:journals/tit/CandesLS15}
 \begin{equation} 			\label{Eqn:wflow_stepsize}
\mu_k = \min \{ 1-e^{-k/k_0}, \mu_{\max} \} \hspace{1cm} k_0 = 330, \ \mu_{\max} = 0.4.
\end{equation}
This descent direction and stepsize choice lead to the wflow algorithm.

\begin{algorithm}[H]
\caption{wflow algorithm}	\label{Alg:wflow}

\begin{algorithmic}[1]
	\Statex 	\textbf{Input:} Sensing operator $\caA$ (\ref{Eqn:A_definition}) with sensing vectors $a_i$ for $i =1, 2, \ldots, m$, observation vector $b \in \bbR^m_+$.
	\Statex 	\textbf{Output:} Approximate solution signal $x$.
	\State 		\textit{Initialize:} Compute the algebraically largest eigenpair $(\lambda_1, v_1)$ of $\caA^*b$, set $x_0 = \alpha v_1$ where $\alpha^2 =  n \sum_{i=1}^mb_i / \sum_{i=1}^m ||a_i||_2^2$, set $r_0 = \frac{1}{m} \left[ \caA(x_{0}x_{0}^*) - b \right]$ and  $k = 0$.
	\While {$||r_k||_2 > \textnormal{tol}$}
		\State	Compute Wirtinger derivative: $\nabla f(x_k)$ from (\ref{Eqn:wflow_derivative}).
		\State	Compute stepsize: $\mu_{k+1}$ from (\ref{Eqn:wflow_stepsize}) and set $\alpha_k = \frac{\mu_{k+1}}{||x_0||_2^2}$.
		\State 	Compute signal update: $x_{k+1} = x_k - \alpha_k \nabla f(x_k)$.
		\State	Compute residual: $r_{k+1} = \frac{1}{m} \left[ \caA(x_{k+1}x_{k+1}^*) - b \right]$.
		\State	$k = k + 1$.
	\EndWhile	\\
	\textit{Return:} $x =  x_k$.
\end{algorithmic}
\end{algorithm}


In \cite[Section 2.3]{DBLP:journals/tit/CandesLS15}, it is observed that the wflow algorithm can be interpreted as a stochastic gradient scheme, where $\nabla f(x)$ is an unbiased estimate of an ideal gradient $\nabla F(x)$, with
\begin{equation}
F(x) = x^* \left( I - \mathbf{x}\mathbf{x}^* \right) x - \frac{3}{4} \left( ||x||_2^2 - 1\right)^2.
\end{equation}
Interestingly, we observe that the wflow algorithm can also be interpreted as an alternating projection HIO-type algorithm, where computation of the Wirtinger derivative (\ref{Eqn:wflow_derivative}) corresponds to steps 3-5 of the GS algorithm and the signal update $x_{k+1} = x_k - \alpha_k \nabla f(x_k)$ corresponds to step 6 of the GS algorithm.  
More precisely, in the last line of (\ref{Eqn:wflow_derivative}) we see $x_k$ is first mapped to the frequency domain (Algorithm \ref{Alg:GS}, step 3).  
This vector is then damped coordinate-wise by the corresponding residual values $\textnormal{Diag}(r_j)$ for $j = 1, \ldots, L$  (Algorithm \ref{Alg:GS}, step 4).  
Finally, the observation is mapped back to the object domain (Algorithm \ref{Alg:GS}, step 5) and damped by $\alpha_k$ to generate the signal update $x_{k+1} = x_k - \alpha_k \nabla f(x_k)$ (similar to the HIO update (\ref{Eqn:HIO_step_6}) which replaces Algorithm \ref{Alg:GS}, step 6).  

Yet unlike other alternating projection algorithms, the wflow algorithm does not require additional frequency domain information or heuristics.  
The wflow algorithm replaces the frequency domain damping in Algorithm \ref{Alg:GS}, step 4 with a damping which uses the observation residual $r = \caA(xx^*) - b$ as an exact, real-time measurement of the frequency domain violations of $x$.  In this view, $\textnormal{Diag}(r_j)$ in (\ref{Eqn:wflow_derivative}) provides a heuristic-free frequency domain damping.  Consequentially, the wflow algorithm merges the computational simplicity of an alternating projection method (Section  \ref{Subsubsec:phase_retrieval-alternating_direction_methods}) with the broad utility of an unstructured optimization method.  Theorem \ref{Thm:wflow_exact_recovery} provides an exact recovery probability for Algorithm \ref{Alg:wflow} applied to noiseless phase retrieval problems.

\begin{theorem} 			\label{Thm:wflow_exact_recovery}
Consider an arbitrary signal $\mathbf{x}$ in $\bbC^n$ and assume the sensing vectors $a_i$ are independent, uniformly distributed on the unit sphere of $\bbC^n$.  Suppose that the number of measurements obeys $m \geq c_0 n \log(n)$, where $c_0$ is a sufficiently large constant.  Then the wflow initial estimate $x_0$ normalized to have squared Euclidean norm equal to $(\sum_{i=1}^m b_i)/m$ has
\begin{equation} 			\label{Eqn:wflow_exact_recovery_dist}
\dist(x_0, \mathbf{x}) := \min\limits_{\theta \in [0, 2\pi]} ||x_0 - e^{i \theta}\mathbf{x} ||_2 \leq \frac{1}{8}||x||_2
\end{equation}
with probability at least $1 - 10e^{-\gamma n} - 8/n^2$, where $\gamma$ is a positive absolute constant.  Additionally, assume the stepsize is a bounded constant $\mu_k = \mu \leq c_1/n$ for some fixed numerical constant $c_1$.  Then there is an event of probability at least $1 - 13e^{- \gamma n} - me^{-1.5m} - 8/n^2$, such that on this event, starting from any initial solution $x_0$ obeying (\ref{Eqn:wflow_exact_recovery_dist}), we have
\begin{equation} 			\label{Eqn:wflow_exact_recovery_rate}
\dist(x_k, \mathbf{x}) \leq \frac{1}{8} ||\mathbf{x}||_2 \left( 1 - \frac{\mu}{4} \right)^{k / 2}.
\end{equation}
\end{theorem}
\begin{proof}
See \cite[Section 7]{DBLP:journals/tit/CandesLS15}.
\end{proof}

Computationally, the wflow algorithm is very efficient at recovering the signal of an oversampled noiseless observation.  Numerical experiments in \cite[Section 4]{DBLP:journals/tit/CandesLS15} demonstrate the effectiveness of the wflow algorithm for $1$-D random signals, $2$-D natural images, and $3$-D molecular structures.  


More generally, it was proved in \cite{sun2016geometric} that if the observation vectors $a_i$ are independent, uniformly distributed on the unit sphere of $\bbC^n$ and there are $m \geq c_0 n \log^3(n)$ measurements, then with probability at least $1-c_0/m$ the function $f(x) = \frac{1}{2m}||\caA(xx^*) - b||_2^2$ has the following properties:

\begin{itemize} 		

\item 
$f$ has no spurious local minima,

\item
all global minima are equal to $\mathbf{x}$ up to some phase constant: $x = e^{i \theta}\mathbf{x}$, and

\item
$f$ has negative directional curvature at all saddle points.

\end{itemize}
Thus when solving (\ref{Eqn:wflow_least-squares}), algorithms such as the wflow algorithm do not require specialized initialization, and are likewise guaranteed to find a global minimum given sufficient oversampling.  


However, the wflow algorithm can diverge when significant noise exists or the oversampling rate is too low (see Appendix \ref{Sec:Appx-Comparison}).
In contrast, the Gauge Dual Descent (GDD) algorithm of Chapter \ref{Sec:PLGD_algo} is guaranteed to converge and thus well suited for unstructured noisy phase retrieval problems.







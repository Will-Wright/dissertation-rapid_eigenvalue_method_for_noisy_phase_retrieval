\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\tocsection {}{}{Abstract}}{v}{section*.1}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{}{Acknowledgments}}{vi}{section*.2}}
\citation{Fienup82}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/siamis/CandesESV13}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{bracewell1986fourier}
\@writefile{toc}{\contentsline {chapter}{\tocchapter {Chapter}{1}{Introduction}}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:Intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{1.1}{Phase Retrieval}}{1}{section.1.1}}
\newlabel{Subsec:phase_retrieval-math_model}{{1.1}{1}{Phase Retrieval}{section.1.1}{}}
\newlabel{Eqn:phase_retrieval}{{1.1.1}{1}{Phase Retrieval}{equation.1.1.1}{}}
\citation{miao1999extending}
\citation{DBLP:journals/siamis/CandesESV13}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{fienup1987phase}
\citation{bunk2007diffractive}
\citation{chai2010array}
\citation{miao2008extending}
\citation{walther1963question}
\citation{harrison1993phase}
\citation{millane1990phase}
\citation{elser2017benchmark}
\citation{DBLP:journals/spm/ShechtmanECCMS15}
\citation{Guizar-Sicairos}
\citation{Guizar-Sicairos}
\citation{duadi2011digital}
\citation{DBLP:journals/siamis/CandesESV13}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\@writefile{toc}{\contentsline {section}{\tocsection {}{1.2}{Applications and Experimental Models}}{2}{section.1.2}}
\newlabel{Subsec:phase_retrieval-applications}{{1.2}{2}{Applications and Experimental Models}{section.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Depiction of coherent diffractive imaging. Coherent waves (left) are projected at an image (center) which causes a diffraction pattern that is measured by a detector (right). Image from \cite  {Guizar-Sicairos}.\relax }}{3}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:CDI}{{1.1}{3}{Depiction of coherent diffractive imaging. Coherent waves (left) are projected at an image (center) which causes a diffraction pattern that is measured by a detector (right). Image from \cite {Guizar-Sicairos}.\relax }{figure.caption.3}{}}
\newlabel{Eqn:FCx}{{1.2.1}{3}{Applications and Experimental Models}{equation.1.2.1}{}}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Results from the Gauge Dual Descent algorithm (Algorithm \ref  {Alg:PGD} in Section \ref  {Subsec:PLGD_algo-algo}) applied to a noisy phase retrieval problem (\ref  {Eqn:phase_retrieval}) with oversampling rate $L = m/n=8$ and noise ratio $\epsilon _\text  {rel} = ||\eta ||_2 / ||b||_2 = 0.30$.\relax }}{4}{figure.caption.4}}
\newlabel{Fig:parrot_signal_iterates}{{1.2}{4}{Results from the Gauge Dual Descent algorithm (Algorithm \ref {Alg:PGD} in Section \ref {Subsec:PLGD_algo-algo}) applied to a noisy phase retrieval problem (\ref {Eqn:phase_retrieval}) with oversampling rate $L = m/n=8$ and noise ratio $\epsilon _\text {rel} = ||\eta ||_2 / ||b||_2 = 0.30$.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{1.3}{Contributions}}{4}{section.1.3}}
\newlabel{Subsec:Intro-contributions}{{1.3}{4}{Contributions}{section.1.3}{}}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{DBLP:journals/siamis/CandesESV13}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{DBLP:journals/tit/CandesLS15}
\@writefile{toc}{\contentsline {section}{\tocsection {}{1.4}{Notation}}{6}{section.1.4}}
\newlabel{Subsec:Intro-notation}{{1.4}{6}{Notation}{section.1.4}{}}
\newlabel{Def:shatten_norms}{{1.4.1}{7}{Notation}{equation.1.4.1}{}}
\newlabel{Def:Frobenius_norm}{{1.4.2}{7}{Notation}{equation.1.4.2}{}}
\newlabel{Def:matrix_norm}{{1.4.3}{7}{Notation}{equation.1.4.3}{}}
\newlabel{Def:indicator_function}{{1.4.6}{8}{Notation}{equation.1.4.6}{}}
\newlabel{Def:normal_cone}{{1.4.7}{8}{Notation}{equation.1.4.7}{}}
\newlabel{Def:eigenvalues}{{1.4.9}{8}{Notation}{equation.1.4.9}{}}
\newlabel{Def:Ritz_pair_val_vec}{{1.4.10}{8}{Notation}{equation.1.4.10}{}}
\newlabel{Def:trace_inner_product}{{1.4.11}{8}{Notation}{equation.1.4.11}{}}
\citation{reed1980functional}
\newlabel{Def:adjoint_operator}{{1.4.12}{9}{Notation}{equation.1.4.12}{}}
\newlabel{Def:subdifferential}{{1.4.15}{9}{Notation}{equation.1.4.15}{}}
\newlabel{Def:Gaussian_distribution}{{1.4.16}{10}{Notation}{equation.1.4.16}{}}
\newlabel{Def:Gaussian_distribution_complex}{{1.4.17}{10}{Notation}{equation.1.4.17}{}}
\citation{DBLP:journals/corr/JaganathanEH15a}
\citation{LeviS84}
\@writefile{toc}{\contentsline {chapter}{\tocchapter {Chapter}{2}{Noisy Phase Retrieval}}{11}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:phase_retrieval}{{2}{11}{Noisy Phase Retrieval}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2.1}{Introduction}}{11}{section.2.1}}
\newlabel{Subsec:phase_retrieval-intro}{{2.1}{11}{Introduction}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2.2}{Alternating Projection Methods}}{11}{section.2.2}}
\newlabel{Subsubsec:phase_retrieval-alternating_direction_methods}{{2.2}{11}{Alternating Projection Methods}{section.2.2}{}}
\citation{GS72}
\citation{Fienup82}
\citation{rodriguez2013oversampling}
\citation{martin2012noise}
\citation{DBLP:journals/corr/JaganathanEH15a}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.2.1}{Gershberg-Saxton Algorithm}}{12}{subsection.2.2.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gershberg-Saxton (GS) algorithm\relax }}{12}{algorithm.1}}
\newlabel{Alg:GS}{{1}{12}{Gershberg-Saxton (GS) algorithm\relax }{algorithm.1}{}}
\citation{Fienup82}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.2.2}{Hybrid Input-Output Algorithm}}{13}{subsection.2.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Phase retrieval as a nonlinear feedback control system\relax }}{13}{figure.caption.5}}
\newlabel{Fig:nonlinear_feedback_control_system}{{2.1}{13}{Phase retrieval as a nonlinear feedback control system\relax }{figure.caption.5}{}}
\newlabel{Eqn:HIO_linear_appx}{{2.2.1}{13}{Hybrid Input-Output Algorithm}{equation.2.2.1}{}}
\citation{DBLP:journals/corr/JaganathanEH15a}
\citation{rodriguez2013oversampling}
\citation{rodriguez2013oversampling}
\citation{martin2012noise}
\newlabel{Eqn:HIO_step_6}{{2.2.2}{14}{Hybrid Input-Output Algorithm}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.2.3}{HIO-Type Methods}}{14}{subsection.2.2.3}}
\citation{DBLP:journals/corr/JaganathanEH15a}
\citation{DBLP:journals/corr/abs-1104-4406}
\citation{shechtman2014gespar}
\citation{cai2016optimal}
\citation{katkovnik2017phase}
\citation{jiang2017robust}
\citation{goldstein2018phasemax}
\citation{bahmani2016phase}
\citation{DBLP:journals/corr/abs-1211-0872}
\citation{DBLP:journals/corr/abs-1104-4406}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2.3}{Structured Optimization Methods}}{15}{section.2.3}}
\newlabel{Subsubsec:phase_retrieval-structured}{{2.3}{15}{Structured Optimization Methods}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.3.1}{Compressive Phase Retrieval Methods}}{15}{subsection.2.3.1}}
\citation{shechtman2014gespar}
\citation{shechtman2014gespar}
\citation{cai2016optimal}
\citation{chen2001atomic}
\citation{zhang2017fast}
\citation{jiang2017robust}
\citation{katkovnik2017phase}
\citation{goldstein2018phasemax}
\citation{bahmani2016phase}
\citation{chen2001atomic}
\citation{candes2006stable}
\citation{goldstein2018phasemax}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.3.2}{Robust Phase Retrieval Methods}}{17}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.3.3}{Supervised Phase Retrieval Methods}}{17}{subsection.2.3.3}}
\newlabel{Eqn:PhaseMax}{{2.3.2}{17}{Supervised Phase Retrieval Methods}{equation.2.3.2}{}}
\citation{DBLP:journals/siamis/CandesESV13}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/tit/CandesLS15}
\citation{sun2016geometric}
\citation{candes2014solving}
\citation{candes2013phaselift}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{DBLP:journals/siamis/CandesESV13}
\citation{candes2013phaselift}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2.4}{Unstructured Optimization Methods}}{18}{section.2.4}}
\newlabel{Subsubsec:phase_retrieval-unstructured}{{2.4}{18}{Unstructured Optimization Methods}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.4.1}{PhaseLift}}{18}{subsection.2.4.1}}
\newlabel{Eqn:A_definition}{{2.4.1}{18}{PhaseLift}{equation.2.4.1}{}}
\citation{natarajan1995sparse}
\citation{recht2010guaranteed}
\citation{DBLP:journals/siamis/CandesESV13}
\citation{candes2013phaselift}
\citation{chen2001atomic}
\citation{candes2006stable}
\newlabel{Eqn:A_definition_with_masks}{{2.4.2}{19}{PhaseLift}{equation.2.4.2}{}}
\newlabel{Eqn:PhaseLift_rank_model}{{2.4.3}{19}{PhaseLift}{equation.2.4.3}{}}
\newlabel{Eqn:PhaseLift}{{2.4.4}{19}{PhaseLift}{equation.2.4.4}{}}
\citation{candes2014solving}
\citation{candes2013phaselift}
\citation{candes2014solving}
\citation{DBLP:journals/siamis/CandesESV13}
\citation{candes2013phaselift}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\newlabel{Eqn:basis_pursuit}{{2.4.5}{20}{PhaseLift}{equation.2.4.5}{}}
\newlabel{Thm:PhaseLift_exact}{{2.4.1}{20}{}{theorem.2.4.1}{}}
\citation{candes2013phaselift}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/tit/CandesLS15}
\newlabel{Eqn:PhaseLift_solution_signal}{{2.4.6}{21}{PhaseLift}{equation.2.4.6}{}}
\newlabel{Thm:PhaseLift_approx}{{2.4.2}{21}{}{theorem.2.4.2}{}}
\newlabel{Eqn:Thm_PhaseLift_approx_X}{{2.4.7}{21}{}{equation.2.4.7}{}}
\newlabel{Eqn:Thm_PhaseLift_approx_x}{{2.4.8}{21}{}{equation.2.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.4.2}{Wflow}}{21}{subsection.2.4.2}}
\newlabel{Eqn:wflow_least-squares}{{2.4.9}{21}{Wflow}{equation.2.4.9}{}}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/tit/CandesLS15}
\newlabel{Eqn:wflow_initialization}{{2.4.10}{22}{Wflow}{equation.2.4.10}{}}
\newlabel{Eqn:wflow_expected_value}{{2.4.11}{22}{Wflow}{equation.2.4.11}{}}
\newlabel{Eqn:wflow_derivative}{{2.4.12}{22}{Wflow}{equation.2.4.12}{}}
\newlabel{Eqn:wflow_stepsize}{{2.4.13}{22}{Wflow}{equation.2.4.13}{}}
\citation{DBLP:journals/tit/CandesLS15}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces wflow algorithm\relax }}{23}{algorithm.2}}
\newlabel{Alg:wflow}{{2}{23}{wflow algorithm\relax }{algorithm.2}{}}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/tit/CandesLS15}
\citation{sun2016geometric}
\newlabel{Thm:wflow_exact_recovery}{{2.4.3}{24}{}{theorem.2.4.3}{}}
\newlabel{Eqn:wflow_exact_recovery_dist}{{2.4.15}{24}{}{equation.2.4.15}{}}
\newlabel{Eqn:wflow_exact_recovery_rate}{{2.4.16}{24}{}{equation.2.4.16}{}}
\citation{rockafellar1970convex}
\citation{DBLP:journals/mp/Freund87}
\citation{DBLP:journals/siamjo/FriedlanderMP14}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{rockafellar1970convex}
\citation{DBLP:journals/mp/Freund87}
\citation{DBLP:journals/siamjo/FriedlanderMP14}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{rockafellar1970convex}
\citation{DBLP:journals/siamjo/FriedlanderMP14}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\@writefile{toc}{\contentsline {chapter}{\tocchapter {Chapter}{3}{Gauge Duality Theory}}{26}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:PLGD}{{3}{26}{Gauge Duality Theory}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3.1}{Introduction}}{26}{section.3.1}}
\newlabel{Subsec:PLGD-intro}{{3.1}{26}{Introduction}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3.2}{Primal-Gauge Dual Models and Background}}{27}{section.3.2}}
\newlabel{Subsec:PLGD-models_intro}{{3.2}{27}{Primal-Gauge Dual Models and Background}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.2.1}{Models and Definitions}}{27}{subsection.3.2.1}}
\newlabel{Eqn:PhaseLift-P-GD}{{3.2.1}{27}{Models and Definitions}{equation.3.2.1}{}}
\newlabel{Eqn:PhaseLift_P_GD_nonlinear_form}{{3.2.2}{27}{Models and Definitions}{equation.3.2.2}{}}
\newlabel{Def:antipolar_set}{{3.2.3}{27}{Models and Definitions}{equation.3.2.3}{}}
\citation{rockafellar1970convex}
\newlabel{Def:polar}{{3.2.4}{28}{Models and Definitions}{equation.3.2.4}{}}
\newlabel{Def:polar_function_1}{{3.2.5}{28}{Models and Definitions}{equation.3.2.5}{}}
\newlabel{Def:polar_function_2}{{3.2.6}{28}{Models and Definitions}{equation.3.2.6}{}}
\newlabel{Def:dual_norm}{{3.2.7}{28}{Models and Definitions}{equation.3.2.7}{}}
\newlabel{Def:preimage}{{3.2.8}{28}{Models and Definitions}{equation.3.2.8}{}}
\citation{rockafellar1970convex}
\citation{rockafellar1970convex}
\citation{rockafellar1970convex}
\citation{rockafellar1970convex}
\citation{rockafellar1970convex}
\newlabel{Def:affine_hull}{{3.2.9}{29}{Models and Definitions}{equation.3.2.9}{}}
\newlabel{Def:relative_interior}{{3.2.10}{29}{Models and Definitions}{equation.3.2.10}{}}
\newlabel{Def:support_function}{{3.2.11}{29}{Models and Definitions}{equation.3.2.11}{}}
\newlabel{Def:Minkowski_function}{{3.2.12}{29}{Models and Definitions}{equation.3.2.12}{}}
\newlabel{Eqn:epigraph}{{3.2.13}{29}{Models and Definitions}{equation.3.2.13}{}}
\newlabel{Def:polar_function_3}{{3.2.14}{29}{Models and Definitions}{equation.3.2.14}{}}
\citation{boyd2004convex}
\citation{rockafellar1970convex}
\citation{ben2001lectures}
\citation{rockafellar1970convex}
\citation{boyd2004convex}
\newlabel{Def:polar_function_4}{{3.2.15}{30}{Models and Definitions}{equation.3.2.15}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.2.2}{Development of Gauge Duality and Relation to Lagrange Duality}}{30}{subsection.3.2.2}}
\newlabel{Def:conjugate_function_1}{{3.2.16}{30}{Development of Gauge Duality and Relation to Lagrange Duality}{equation.3.2.16}{}}
\newlabel{Def:conjugate_function_2}{{3.2.17}{30}{Development of Gauge Duality and Relation to Lagrange Duality}{equation.3.2.17}{}}
\newlabel{Def:dual_cone}{{3.2.18}{30}{Development of Gauge Duality and Relation to Lagrange Duality}{equation.3.2.18}{}}
\citation{rockafellar1970convex}
\citation{DBLP:journals/mp/Freund87}
\citation{DBLP:journals/siamjo/FriedlanderMP14}
\citation{DBLP:journals/siamjo/FriedlanderMP14}
\citation{aravkin2017foundations}
\newlabel{Eqn:PhaseLift_P_GD_linear_form}{{3.2.20}{31}{Development of Gauge Duality and Relation to Lagrange Duality}{equation.3.2.20}{}}
\newlabel{Eqn:PhaseLift_P_GD_inequality_form}{{3.2.21}{31}{Development of Gauge Duality and Relation to Lagrange Duality}{equation.3.2.21}{}}
\citation{rockafellar1970convex}
\citation{DBLP:journals/mp/Freund87}
\citation{DBLP:journals/siamjo/FriedlanderMP14}
\citation{aravkin2017foundations}
\citation{DBLP:journals/siamjo/FriedlanderMP14}
\citation{rockafellar1970convex}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3.3}{The Gauge Duality Theorem and Optimality Conditions}}{32}{section.3.3}}
\newlabel{Subsec:PLGD-theory}{{3.3}{32}{The Gauge Duality Theorem and Optimality Conditions}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Dependency chart for proofs (i) and (ii) of Theorem \ref  {Thm:P-GD-inequality_pair_are_duals} which establish the gauge duality of P-GD-ineq (\ref  {Eqn:PhaseLift_P_GD_inequality_form}). \relax }}{32}{figure.caption.6}}
\newlabel{Fig:gauge_duality_thm_flowchart}{{3.1}{32}{Dependency chart for proofs (i) and (ii) of Theorem \ref {Thm:P-GD-inequality_pair_are_duals} which establish the gauge duality of P-GD-ineq (\ref {Eqn:PhaseLift_P_GD_inequality_form}). \relax }{figure.caption.6}{}}
\citation{DBLP:journals/siamjo/FriedlanderMP14}
\citation{DBLP:journals/siamjo/FriedlanderMP14}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.3.1}{Supporting Propositions}}{33}{subsection.3.3.1}}
\newlabel{Subsubsec:PLGD-theory_props}{{3.3.1}{33}{Supporting Propositions}{subsection.3.3.1}{}}
\newlabel{Prop:antipolar_set_equalities}{{3.3.1}{33}{}{thm.3.3.1}{}}
\newlabel{Eqn:antipolar_set_equalities_1}{{3.3.1}{33}{}{equation.3.3.1}{}}
\newlabel{Eqn:antipolar_set_equalities_2}{{3.3.2}{33}{}{equation.3.3.2}{}}
\newlabel{Prop:antipolar_of_primal_phase_retrieval}{{3.3.2}{33}{}{thm.3.3.2}{}}
\newlabel{Eqn:antipolar_of_primal_phase_retrieval}{{3.3.3}{33}{}{equation.3.3.3}{}}
\newlabel{Eqn:antipolar_eqn_for_proof_1}{{3.3.4}{34}{Supporting Propositions}{equation.3.3.4}{}}
\newlabel{Eqn:antipolar_eqn_for_proof_2}{{3.3.5}{34}{Supporting Propositions}{equation.3.3.5}{}}
\newlabel{Prop:P-GD-Minkowski_set_unique}{{3.3.3}{34}{}{thm.3.3.3}{}}
\newlabel{Prop:P-GD-polar_of_sum_of_gauges_sets_equal}{{3.3.4}{35}{}{thm.3.3.4}{}}
\newlabel{Prop:P-GD-polar_of_sum_of_gauges}{{3.3.5}{35}{}{thm.3.3.5}{}}
\newlabel{Eqn:polar_of_sum_of_gauges}{{3.3.7}{36}{}{equation.3.3.7}{}}
\newlabel{Prop:P-GD-indicator_epigraph_polar}{{3.3.6}{36}{}{thm.3.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.3.2}{Gauge Duality Theorem}}{37}{subsection.3.3.2}}
\newlabel{Subsubsec:PLGD-theory_thm}{{3.3.2}{37}{Gauge Duality Theorem}{subsection.3.3.2}{}}
\newlabel{Thm:P-GD-inequality_pair_are_duals}{{3.3.1}{37}{}{theorem.3.3.1}{}}
\newlabel{Eqn:P-GD-Thm_duals-eqn1}{{3.3.9}{38}{Gauge Duality Theorem}{equation.3.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.3.3}{Weak Duality, Strong Duality, and Optimality Conditions}}{39}{subsection.3.3.3}}
\newlabel{Subsubsec:PLGD-theory_weak-strong-opt}{{3.3.3}{39}{Weak Duality, Strong Duality, and Optimality Conditions}{subsection.3.3.3}{}}
\newlabel{Prop:weak_duality}{{3.3.7}{39}{}{thm.3.3.7}{}}
\newlabel{Eqn:weak_duality}{{3.3.14}{39}{}{equation.3.3.14}{}}
\citation{rockafellar1970convex}
\newlabel{Eqn:weak_duality_inequalities}{{3.3.15}{40}{Weak Duality, Strong Duality, and Optimality Conditions}{equation.3.3.15}{}}
\newlabel{Eqn:PhaseLift_P_GD_ineq_Lagrange_dual}{{3.3.16}{40}{Weak Duality, Strong Duality, and Optimality Conditions}{equation.3.3.16}{}}
\newlabel{Prop:strong_duality}{{3.3.8}{40}{}{thm.3.3.8}{}}
\newlabel{Eqn:strong_duality_1}{{3.3.17}{40}{}{equation.3.3.17}{}}
\newlabel{Eqn:strong_duality_2}{{3.3.19}{40}{}{equation.3.3.19}{}}
\citation{rockafellar1970convex}
\citation{rockafellar1970convex}
\citation{rockafellar1970convex}
\newlabel{Eqn:strong_duality_3}{{3.3.20}{41}{}{equation.3.3.20}{}}
\newlabel{Prop:opt_conds}{{3.3.9}{42}{}{thm.3.3.9}{}}
\newlabel{Eqn:Prop_opt_conds}{{3.3.21}{42}{}{equation.3.3.21}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3.4}{Theory Applied to the PLGD Model}}{43}{section.3.4}}
\newlabel{Subsec:PLGD-opt_conds_primal_recovery}{{3.4}{43}{Theory Applied to the PLGD Model}{section.3.4}{}}
\newlabel{Eqn:kappa_polar}{{3.4.1}{43}{Theory Applied to the PLGD Model}{equation.3.4.1}{}}
\citation{grigorieff1991note}
\newlabel{Eqn:von_Neumann_trace_inequality}{{3.4.2}{44}{Theory Applied to the PLGD Model}{equation.3.4.2}{}}
\newlabel{Cor:PLGD-optimality}{{3.4.1}{44}{}{thm.3.4.1}{}}
\newlabel{Eqn:von_Neumann_in_proof}{{3.4.3}{45}{Theory Applied to the PLGD Model}{equation.3.4.3}{}}
\newlabel{Prop:PLGD-opt_unconstrained}{{3.4.2}{45}{}{thm.3.4.2}{}}
\newlabel{Eqn:subdiff_optimality_prop}{{3.4.4}{45}{}{equation.3.4.4}{}}
\citation{rockafellar1970convex}
\newlabel{Cor:PLGD-primal_recovery_refinement}{{3.4.3}{46}{}{thm.3.4.3}{}}
\newlabel{Eqn:PLGD-primal_recovery_refinement_pf_eqn_1}{{3.4.8}{47}{Theory Applied to the PLGD Model}{equation.3.4.8}{}}
\newlabel{Eqn:PLGD-primal_recovery_refinement_pf_eqn_2}{{3.4.9}{47}{Theory Applied to the PLGD Model}{equation.3.4.9}{}}
\newlabel{Eqn:PLGD-noise_is_dual_variable}{{3.4.10}{48}{Theory Applied to the PLGD Model}{equation.3.4.10}{}}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\@writefile{toc}{\contentsline {chapter}{\tocchapter {Chapter}{4}{The Gauge Dual Descent Algorithm for the PLGD Model}}{49}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:PLGD_algo}{{4}{49}{The Gauge Dual Descent Algorithm for the PLGD Model}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{4.1}{Introduction}}{49}{section.4.1}}
\newlabel{Subsec:PLGD_algo-intro}{{4.1}{49}{Introduction}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{4.2}{The Gauge Dual Descent Algorithm}}{49}{section.4.2}}
\newlabel{Subsec:PLGD_algo-algo}{{4.2}{49}{The Gauge Dual Descent Algorithm}{section.4.2}{}}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{parikh2014proximal}
\newlabel{Eqn:GD-subdifferential}{{4.2.1}{50}{The Gauge Dual Descent Algorithm}{equation.4.2.1}{}}
\newlabel{Eqn:GD-gradient}{{4.2.2}{50}{The Gauge Dual Descent Algorithm}{equation.4.2.2}{}}
\citation{barzilai1988twopoint}
\citation{nocedal2006numerical}
\citation{zhang2004nonmonotone}
\citation{bertsekas2016nonlinear}
\newlabel{Eqn:GD-diff_tol}{{4.2.3}{51}{The Gauge Dual Descent Algorithm}{equation.4.2.3}{}}
\newlabel{Eqn:Barzilai-Borwein_steplength}{{4.2.4}{51}{The Gauge Dual Descent Algorithm}{equation.4.2.4}{}}
\newlabel{Eqn:GD-linesearch}{{4.2.5}{51}{The Gauge Dual Descent Algorithm}{equation.4.2.5}{}}
\newlabel{Eqn:GD-steplength_sum}{{4.2.6}{51}{The Gauge Dual Descent Algorithm}{equation.4.2.6}{}}
\citation{boyd2004convex}
\newlabel{Eqn:GD-projection}{{4.2.7}{52}{The Gauge Dual Descent Algorithm}{equation.4.2.7}{}}
\newlabel{Eqn:GD-projection_2}{{4.2.8}{52}{The Gauge Dual Descent Algorithm}{equation.4.2.8}{}}
\newlabel{Eqn:GD-primal_rec1}{{4.2.9}{52}{The Gauge Dual Descent Algorithm}{equation.4.2.9}{}}
\newlabel{Eqn:GD-primal_rec2}{{4.2.10}{52}{The Gauge Dual Descent Algorithm}{equation.4.2.10}{}}
\citation{bertsekas2016nonlinear}
\newlabel{Eqn:GD-PFD}{{4.2.11}{53}{The Gauge Dual Descent Algorithm}{equation.4.2.11}{}}
\newlabel{Eqn:GD-DFP}{{4.2.12}{53}{The Gauge Dual Descent Algorithm}{equation.4.2.12}{}}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Gauge Dual Descent (GDD) algorithm\relax }}{54}{algorithm.3}}
\newlabel{Alg:PGD}{{3}{54}{Gauge Dual Descent (GDD) algorithm\relax }{algorithm.3}{}}
\newlabel{Eqn:saga_conv_crit_primal}{{4.2.13}{55}{The Gauge Dual Descent Algorithm}{equation.4.2.13}{}}
\newlabel{Eqn:saga_conv_crit_gap}{{4.2.14}{55}{The Gauge Dual Descent Algorithm}{equation.4.2.14}{}}
\citation{schmidt2005minFunc}
\citation{schmidt2008minConf}
\citation{schmidt2009optimizing}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{zhang2004nonmonotone}
\citation{bertsekas2016nonlinear}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\newlabel{Eqn:GD-steplength}{{4.2.15}{56}{The Gauge Dual Descent Algorithm}{equation.4.2.15}{}}
\citation{sun2016geometric}
\@writefile{toc}{\contentsline {section}{\tocsection {}{4.3}{Noiseless Phase Retrieval}}{57}{section.4.3}}
\newlabel{Subsec:PLGD_algo-noiseless_success}{{4.3}{57}{Noiseless Phase Retrieval}{section.4.3}{}}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The GDD algorithm (Algorithm \ref  {Alg:PGD}) with and without primal (\ref  {Eqn:GD-PFD}) and dual refinement (\ref  {Eqn:GD-DFP}). This experiment involves a random gaussian signal of size $n = 128$ with $L = 10$ observations and no noise. Both termination conditions (\ref  {Eqn:saga_conv_crit_primal}) and (\ref  {Eqn:saga_conv_crit_gap}) are required. Signal relative error is measured as $|| \mathbf  {x}\mathbf  {x}^*-x_0x_0^* ||_F / ||\mathbf  {x}||_2^2$.\relax }}{58}{table.caption.7}}
\newlabel{Tab:noiseless_runtimes}{{4.1}{58}{The GDD algorithm (Algorithm \ref {Alg:PGD}) with and without primal (\ref {Eqn:GD-PFD}) and dual refinement (\ref {Eqn:GD-DFP}). This experiment involves a random gaussian signal of size $n = 128$ with $L = 10$ observations and no noise. Both termination conditions (\ref {Eqn:saga_conv_crit_primal}) and (\ref {Eqn:saga_conv_crit_gap}) are required. Signal relative error is measured as $|| \mathbf {x}\mathbf {x}^*-x_0x_0^* ||_F / ||\mathbf {x}||_2^2$.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {chapter}{\tocchapter {Chapter}{5}{Algorithm Stagnation for Noisy Phase Retrieval}}{60}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:PLGD_term_crit}{{5}{60}{Algorithm Stagnation for Noisy Phase Retrieval}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{5.1}{Introduction}}{60}{section.5.1}}
\newlabel{Subsec:PLGD_term_crit-intro}{{5.1}{60}{Introduction}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{5.2}{Experimental Models and Residuals}}{60}{section.5.2}}
\newlabel{Subsec:PLGD_term_crit-NOISY_MODELS_AND_RESIDUALS}{{5.2}{60}{Experimental Models and Residuals}{section.5.2}{}}
\newlabel{Def:Gaussian_noise}{{5.2.1}{61}{Experimental Models and Residuals}{equation.5.2.1}{}}
\newlabel{Eqn:phase_retrieval_Gaussian_noise}{{5.2.2}{61}{Experimental Models and Residuals}{equation.5.2.2}{}}
\newlabel{Eqn:PhaseLift-GD_Gaussian_noise}{{5.2.3}{61}{Experimental Models and Residuals}{equation.5.2.3}{}}
\newlabel{Def:synthetic_noise}{{5.2.4}{62}{Experimental Models and Residuals}{equation.5.2.4}{}}
\newlabel{Eqn:phase_retrieval_synthetic_noise}{{5.2.5}{62}{Experimental Models and Residuals}{equation.5.2.5}{}}
\newlabel{Eqn:PhaseLift-GD_synthetic_noise}{{5.2.6}{62}{Experimental Models and Residuals}{equation.5.2.6}{}}
\newlabel{Eqn:term_crit-candidate_residuals}{{5.2.7}{63}{Experimental Models and Residuals}{equation.5.2.7}{}}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\@writefile{toc}{\contentsline {section}{\tocsection {}{5.3}{Algorithm Stagnation}}{64}{section.5.3}}
\newlabel{Subsec:PLGD_term_crit-stagnation}{{5.3}{64}{Algorithm Stagnation}{section.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Number of iterations and signal relative error (\ref  {Eqn:term_crit-candidate_residuals}a) (xErr) for the GDD algorithm (Algorithm \ref  {Alg:PGD}) applied to the PLGD model with synthetic noise (\ref  {Eqn:PhaseLift-GD_synthetic_noise}). Signal size is $n=128$, with various noise ratios $\epsilon _\textnormal  {rel}$ and oversampling values $L$. The convergence conditions (\ref  {Eqn:saga_conv_crit_primal}) and (\ref  {Eqn:saga_conv_crit_gap}) are set to tolerances $\textnormal  {tol}_\textnormal  {feas} = \textnormal  {tol}_\textnormal  {gap} = 2 \times 10^{-4}$. Note that each signal relative error in Table \ref  {Tab:synthetic_dual_variable_converges} was 1-3 orders of magnitude smaller than the relative error of the signal returned by the wflow algorithm (Algorithm \ref  {Alg:wflow}) for the same problem.\relax }}{65}{table.caption.8}}
\newlabel{Tab:synthetic_dual_variable_converges}{{5.1}{65}{Number of iterations and signal relative error (\ref {Eqn:term_crit-candidate_residuals}a) (xErr) for the GDD algorithm (Algorithm \ref {Alg:PGD}) applied to the PLGD model with synthetic noise (\ref {Eqn:PhaseLift-GD_synthetic_noise}). Signal size is $n=128$, with various noise ratios $\epsilon _\rel $ and oversampling values $L$. The convergence conditions (\ref {Eqn:saga_conv_crit_primal}) and (\ref {Eqn:saga_conv_crit_gap}) are set to tolerances $\textnormal {tol}_\textnormal {feas} = \textnormal {tol}_\textnormal {gap} = 2 \times 10^{-4}$. Note that each signal relative error in Table \ref {Tab:synthetic_dual_variable_converges} was 1-3 orders of magnitude smaller than the relative error of the signal returned by the wflow algorithm (Algorithm \ref {Alg:wflow}) for the same problem.\relax }{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Final duality gap (\ref  {Eqn:term_crit-candidate_residuals}f) (duGap) and signal relative error (\ref  {Eqn:term_crit-candidate_residuals}a) (xErr) for the GDD algorithm (Algorithm \ref  {Alg:PGD}) applied to PLGD models with Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}). The GDD algorithm failed to converge after 1,000 iterations for all problems. Signal size is $n=128$, with various noise ratios $\epsilon _\textnormal  {rel}$ and oversampling values $L$. The convergence conditions (\ref  {Eqn:saga_conv_crit_primal}) and (\ref  {Eqn:saga_conv_crit_gap}) are set to tolerances $\textnormal  {tol}_\textnormal  {feas} = \textnormal  {tol}_\textnormal  {gap} = 2 \times 10^{-4}$.\relax }}{65}{table.caption.9}}
\newlabel{Tab:Gaussian_dual_variable_stagnates}{{5.2}{65}{Final duality gap (\ref {Eqn:term_crit-candidate_residuals}f) (duGap) and signal relative error (\ref {Eqn:term_crit-candidate_residuals}a) (xErr) for the GDD algorithm (Algorithm \ref {Alg:PGD}) applied to PLGD models with Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}). The GDD algorithm failed to converge after 1,000 iterations for all problems. Signal size is $n=128$, with various noise ratios $\epsilon _\rel $ and oversampling values $L$. The convergence conditions (\ref {Eqn:saga_conv_crit_primal}) and (\ref {Eqn:saga_conv_crit_gap}) are set to tolerances $\textnormal {tol}_\textnormal {feas} = \textnormal {tol}_\textnormal {gap} = 2 \times 10^{-4}$.\relax }{table.caption.9}{}}
\citation{toh1999sdpt3}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Primal relative error (\ref  {Eqn:term_crit-candidate_residuals}d), dual relative error (\ref  {Eqn:term_crit-candidate_residuals}b), and duality gap (\ref  {Eqn:term_crit-candidate_residuals}f) for 10,000 iterates of the GDD algorithm (Algorithm \ref  {Alg:PGD}) applied to a natural noise model with $n=16$, $L=6$ observations, and noise ratio $0.30$. The horizontal axis is log-plotted to highlight early progress along with later stagnation. The pair $(X_\star , y_\star )$ are computed with \texttt  {SDPT3}.\relax }}{66}{figure.caption.10}}
\newlabel{Fig:noisy_random_relative_errors_stagnate}{{5.1}{66}{Primal relative error (\ref {Eqn:term_crit-candidate_residuals}d), dual relative error (\ref {Eqn:term_crit-candidate_residuals}b), and duality gap (\ref {Eqn:term_crit-candidate_residuals}f) for 10,000 iterates of the GDD algorithm (Algorithm \ref {Alg:PGD}) applied to a natural noise model with $n=16$, $L=6$ observations, and noise ratio $0.30$. The horizontal axis is log-plotted to highlight early progress along with later stagnation. The pair $(X_\star , y_\star )$ are computed with \texttt {SDPT3}.\relax }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces The GDD algorithm (Algorithm \ref  {Alg:PGD}) results and optimal matrix rank for PLGD models with synthetic (\ref  {Eqn:PhaseLift-GD_synthetic_noise}) and Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}). This table depicts the mean rank of $X_\star $, number of the GDD algorithm iterations, and final duality gap (\ref  {Eqn:term_crit-candidate_residuals}f) (duGap) for 100 random phase retrieval models with signal size $n=16$, noise ratio $\epsilon _\textnormal  {rel}$, and oversampling $L$. In all synthetic noise models (\ref  {Eqn:PhaseLift-GD_synthetic_noise}), the solution $X_\star $ was rank-one and the GDD algorithm converged. In all Gaussian noise models (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}), the algorithm reached the maximum of 1,000 iterations without attaining the termination condition (\ref  {Eqn:saga_conv_crit_gap}). Note that pairs $(X_\star , y_\star )$ are computed with \texttt  {SDPT3} and numbers $n_{-k}$ are shorthand for $n \times 10^{-k}$.\relax }}{67}{table.caption.11}}
\newlabel{Tab:average_rank_soln_matrix_with_gaussian_dual_variable}{{5.3}{67}{The GDD algorithm (Algorithm \ref {Alg:PGD}) results and optimal matrix rank for PLGD models with synthetic (\ref {Eqn:PhaseLift-GD_synthetic_noise}) and Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}). This table depicts the mean rank of $X_\star $, number of the GDD algorithm iterations, and final duality gap (\ref {Eqn:term_crit-candidate_residuals}f) (duGap) for 100 random phase retrieval models with signal size $n=16$, noise ratio $\epsilon _\rel $, and oversampling $L$. In all synthetic noise models (\ref {Eqn:PhaseLift-GD_synthetic_noise}), the solution $X_\star $ was rank-one and the GDD algorithm converged. In all Gaussian noise models (\ref {Eqn:PhaseLift-GD_Gaussian_noise}), the algorithm reached the maximum of 1,000 iterations without attaining the termination condition (\ref {Eqn:saga_conv_crit_gap}). Note that pairs $(X_\star , y_\star )$ are computed with \texttt {SDPT3} and numbers $n_{-k}$ are shorthand for $n \times 10^{-k}$.\relax }{table.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces A comparison of the GDD algorithm (Algorithm \ref  {Alg:PGD}) with and without dual refinement (\ref  {Eqn:GD-DFP}) for PLGD models with Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}) with various noise levels, with random Gaussian signal of size $n = 128$ and $L = 10$ observations. The solid blue line indicates dual refinement, and the dashed red line indicates no dual refinement.\relax }}{68}{figure.caption.12}}
\newlabel{Fig:noisy_random_DFP_vs_no_DFP}{{5.2}{68}{A comparison of the GDD algorithm (Algorithm \ref {Alg:PGD}) with and without dual refinement (\ref {Eqn:GD-DFP}) for PLGD models with Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}) with various noise levels, with random Gaussian signal of size $n = 128$ and $L = 10$ observations. The solid blue line indicates dual refinement, and the dashed red line indicates no dual refinement.\relax }{figure.caption.12}{}}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\@writefile{toc}{\contentsline {section}{\tocsection {}{5.4}{New Termination Conditions}}{69}{section.5.4}}
\newlabel{Subsec:PLGD_term_crit-new_term_crit}{{5.4}{69}{New Termination Conditions}{section.5.4}{}}
\newlabel{Eqn:term_crit_new-primal_difference}{{5.4.1}{69}{New Termination Conditions}{equation.5.4.1}{}}
\newlabel{Eqn:term_crit_new-dual_difference}{{5.4.2}{69}{New Termination Conditions}{equation.5.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Signal relative errors (\ref  {Eqn:term_crit-candidate_residuals}a) for six PLGD models with Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}) solved with the GDD algorithm (Algorithm \ref  {Alg:PGD}). The image in Figure \ref  {Fig:parrot_signal_iterates} was resized to $64 \times 64$ pixels, made grayscale, and six models were generated with oversampling rates of $L = 5, 10$ and noise ratios $\epsilon _\textnormal  {rel}= 0.05, 0.15, 0.30$. These models were then solved using the GDD algorithm set to terminate after 1,000 iterations. The red circle (where present) indicates when the dual objective was determined nondifferentiable.\relax }}{70}{figure.caption.13}}
\newlabel{Fig:term_crit-signal_err}{{5.3}{70}{Signal relative errors (\ref {Eqn:term_crit-candidate_residuals}a) for six PLGD models with Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}) solved with the GDD algorithm (Algorithm \ref {Alg:PGD}). The image in Figure \ref {Fig:parrot_signal_iterates} was resized to $64 \times 64$ pixels, made grayscale, and six models were generated with oversampling rates of $L = 5, 10$ and noise ratios $\epsilon _\rel = 0.05, 0.15, 0.30$. These models were then solved using the GDD algorithm set to terminate after 1,000 iterations. The red circle (where present) indicates when the dual objective was determined nondifferentiable.\relax }{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Iterate at which the GDD algorithm (Algorithm \ref  {Alg:PGD}) became primal feasible for models from Figure \ref  {Fig:term_crit-signal_err}.\relax }}{71}{table.caption.14}}
\newlabel{Tab:term_crit-pr_feas_faster_for_big_L_noise}{{5.4}{71}{Iterate at which the GDD algorithm (Algorithm \ref {Alg:PGD}) became primal feasible for models from Figure \ref {Fig:term_crit-signal_err}.\relax }{table.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Intervals of iterates at which the GDD algorithm (Algorithm \ref  {Alg:PGD}) appears to stagnate for models from Figure \ref  {Fig:term_crit-signal_err}.\relax }}{71}{table.caption.15}}
\newlabel{Tab:term_crit-desired_termination_windows}{{5.5}{71}{Intervals of iterates at which the GDD algorithm (Algorithm \ref {Alg:PGD}) appears to stagnate for models from Figure \ref {Fig:term_crit-signal_err}.\relax }{table.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Plots of tolerance values against the iterate at which the GDD algorithm (Algorithm \ref  {Alg:PGD}) first satisfies this tolerance for the models discussed in Figure \ref  {Fig:term_crit-signal_err}. Tolerances depicted are difference values for the primal objective (\ref  {Eqn:term_crit-candidate_residuals}e) and dual variable (\ref  {Eqn:term_crit-candidate_residuals}i). Red circles are placed at tolerances $10^{-n}$. The blue rectangles indicate the proposed intervals of termination from Table \ref  {Tab:term_crit-desired_termination_windows}.\relax }}{72}{figure.caption.16}}
\newlabel{Fig:term_crit-pr_and_dual}{{5.4}{72}{Plots of tolerance values against the iterate at which the GDD algorithm (Algorithm \ref {Alg:PGD}) first satisfies this tolerance for the models discussed in Figure \ref {Fig:term_crit-signal_err}. Tolerances depicted are difference values for the primal objective (\ref {Eqn:term_crit-candidate_residuals}e) and dual variable (\ref {Eqn:term_crit-candidate_residuals}i). Red circles are placed at tolerances $10^{-n}$. The blue rectangles indicate the proposed intervals of termination from Table \ref {Tab:term_crit-desired_termination_windows}.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Signal relative error (\ref  {Eqn:term_crit-candidate_residuals}a) and duality gap (\ref  {Eqn:term_crit-candidate_residuals}f) for the model from Figure \ref  {Fig:term_crit-signal_err} with oversampling $L = 10$ and noise ratio $\epsilon _\textnormal  {rel}= 0.05$. Red circles indicate signals with relative error at least $0.05\%$ above the mean of their ten neighbors, and the black asterisk indicates both the final iterate based on new termination conditions and the optimal iterate based on minimum duality gap.\relax }}{74}{figure.caption.17}}
\newlabel{Fig:term_crit-duality_gap}{{5.5}{74}{Signal relative error (\ref {Eqn:term_crit-candidate_residuals}a) and duality gap (\ref {Eqn:term_crit-candidate_residuals}f) for the model from Figure \ref {Fig:term_crit-signal_err} with oversampling $L = 10$ and noise ratio $\epsilon _\rel = 0.05$. Red circles indicate signals with relative error at least $0.05\%$ above the mean of their ten neighbors, and the black asterisk indicates both the final iterate based on new termination conditions and the optimal iterate based on minimum duality gap.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Iterate at which the GDD algorithm (Algorithm \ref  {Alg:PGD}) terminates for the models from Figure \ref  {Fig:term_crit-signal_err} based on new termination conditions.\relax }}{75}{figure.caption.18}}
\newlabel{Fig:term_crit-model_term_for_tols}{{5.6}{75}{Iterate at which the GDD algorithm (Algorithm \ref {Alg:PGD}) terminates for the models from Figure \ref {Fig:term_crit-signal_err} based on new termination conditions.\relax }{figure.caption.18}{}}
\citation{sorensen1992implicit}
\citation{bai2000eigenvaluetemplates}
\@writefile{toc}{\contentsline {chapter}{\tocchapter {Chapter}{6}{Evolving Matrix Eigenvalue Computation}}{76}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:evol_mats}{{6}{76}{Evolving Matrix Eigenvalue Computation}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{6.1}{Introduction}}{76}{section.6.1}}
\newlabel{Subsec:evol_mats-intro}{{6.1}{76}{Introduction}{section.6.1}{}}
\citation{saad2016analysis}
\citation{comon1990tracking}
\citation{stewart1992updating}
\citation{yang1995projection}
\citation{doukopoulos2008fast}
\citation{ngo2012scaled}
\citation{saad2010numerical}
\@writefile{toc}{\contentsline {section}{\tocsection {}{6.2}{The Evolving Matrix Eigenvalue Problem}}{77}{section.6.2}}
\newlabel{Subsec:evol_mats-spectral_props}{{6.2}{77}{The Evolving Matrix Eigenvalue Problem}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.2.1}{EMEP Definition}}{77}{subsection.6.2.1}}
\newlabel{Subsubsec:evol_mats-EMEP_definition}{{6.2.1}{77}{EMEP Definition}{subsection.6.2.1}{}}
\newlabel{Eqn:EMEP_linesearch_prob_in_evol_mats_chapter}{{6.2.1}{77}{EMEP Definition}{equation.6.2.1}{}}
\citation{DBLP:journals/tit/CandesLS15}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\newlabel{Eqn:EMEP_PLGD}{{6.2.2}{78}{EMEP Definition}{equation.6.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.2.2}{Computational Costs}}{78}{subsection.6.2.2}}
\newlabel{Subsubsec:evol_mats-EMEP_compu_costs}{{6.2.2}{78}{Computational Costs}{subsection.6.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces The GDD algorithm (Algorithm \ref  {Alg:PGD}) runtime and number of DFTs (with percentage of the total in parentheses) for the EMEP (\ref  {Eqn:EMEP_PLGD}), primal refinement (solving (\ref  {Eqn:GD-PFD}) in step 11) and all other operations. Here $n$ is signal size (i.e., number of pixels squared in the image from Figure \ref  {Fig:parrot_signal_iterates}), $L$ is number of observations, and $\epsilon _\textnormal  {rel}$ is the noise ratio.\relax }}{79}{table.caption.19}}
\newlabel{Tab:EMEP_costs}{{6.1}{79}{The GDD algorithm (Algorithm \ref {Alg:PGD}) runtime and number of DFTs (with percentage of the total in parentheses) for the \emep , primal refinement (solving (\ref {Eqn:GD-PFD}) in step 11) and all other operations. Here $n$ is signal size (i.e., number of pixels squared in the image from Figure \ref {Fig:parrot_signal_iterates}), $L$ is number of observations, and $\epsilon _\textnormal {rel}$ is the noise ratio.\relax }{table.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Number of matrix-vector products for each iteration in the EMEP (\ref  {Eqn:EMEP_PLGD})\ for the six smaller models from Table \ref  {Tab:EMEP_costs} .\relax }}{80}{figure.caption.20}}
\newlabel{Fig:EMEP_costs_num_mat_vecs}{{6.1}{80}{Number of matrix-vector products for each iteration in the \emep \ for the six smaller models from Table \ref {Tab:EMEP_costs} .\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.2.3}{Evolving Spectrum Distribution}}{80}{subsection.6.2.3}}
\newlabel{Subsubsec:evol_mats-EMEP_spectrum_and_clustering}{{6.2.3}{80}{Evolving Spectrum Distribution}{subsection.6.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Spectrum of specific EMEP (\ref  {Eqn:EMEP_PLGD})\ matrix iterates $A_k$ for the model from Table \ref  {Tab:EMEP_costs} with signal size $n = 4,096$, oversampling $L = 5$, and noise ratio $\epsilon _\text  {rel} = 0.15$.\relax }}{81}{figure.caption.21}}
\newlabel{Fig:EMEP_full_spectrum}{{6.2}{81}{Spectrum of specific \emep \ matrix iterates $A_k$ for the model from Table \ref {Tab:EMEP_costs} with signal size $n = 4,096$, oversampling $L = 5$, and noise ratio $\epsilon _\text {rel} = 0.15$.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Twenty algebraically largest eigenvalues of specific EMEP (\ref  {Eqn:EMEP_PLGD})\ matrix iterates $A_k$ for the model from Table \ref  {Tab:EMEP_costs} with signal size $n = 4,096$, oversampling $L = 5$, and noise ratio $\epsilon _\text  {rel} = 0.15$.\relax }}{82}{figure.caption.22}}
\newlabel{Fig:EMEP_largest_eigvals}{{6.3}{82}{Twenty algebraically largest eigenvalues of specific \emep \ matrix iterates $A_k$ for the model from Table \ref {Tab:EMEP_costs} with signal size $n = 4,096$, oversampling $L = 5$, and noise ratio $\epsilon _\text {rel} = 0.15$.\relax }{figure.caption.22}{}}
\citation{sorensen1992implicit}
\citation{sorensen1997implicitly}
\citation{golub2012matrix}
\@writefile{toc}{\contentsline {section}{\tocsection {}{6.3}{The Implicitly Restarted Arnoldi Method}}{83}{section.6.3}}
\newlabel{Subsec:evol_mats-IRAM}{{6.3}{83}{The Implicitly Restarted Arnoldi Method}{section.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Dependency chart for the IRAM.\relax }}{83}{figure.caption.23}}
\newlabel{Fig:IRAM_flowchart}{{6.4}{83}{Dependency chart for the IRAM.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.3.1}{The Power Method}}{83}{subsection.6.3.1}}
\newlabel{Subsubsec:evol_mats-power_method}{{6.3.1}{83}{The Power Method}{subsection.6.3.1}{}}
\citation{golub2012matrix}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Power method\relax }}{84}{algorithm.4}}
\newlabel{Alg:power_method}{{4}{84}{Power method\relax }{algorithm.4}{}}
\newlabel{Thm:power_method_conv_rate}{{6.3.1}{84}{}{theorem.6.3.1}{}}
\citation{golub2012matrix}
\citation{golub2012matrix}
\newlabel{Eqn:power_method_conv_rate_1}{{6.3.1}{85}{}{equation.6.3.1}{}}
\newlabel{Eqn:power_method_conv_rate_2}{{6.3.2}{85}{}{equation.6.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.3.2}{The $m$-Step Arnoldi Iteration}}{85}{subsection.6.3.2}}
\newlabel{Subsubsec:evol_mats-Arnoldi}{{6.3.2}{85}{The $m$-Step Arnoldi Iteration}{subsection.6.3.2}{}}
\newlabel{Def:krylov_subspace}{{6.3.3}{85}{The $m$-Step Arnoldi Iteration}{equation.6.3.3}{}}
\citation{golub2012matrix}
\citation{saad2011numerical}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces $m$-step Arnoldi iteration\relax }}{86}{algorithm.5}}
\newlabel{Alg:Arnoldi_iteration}{{5}{86}{$m$-step Arnoldi iteration\relax }{algorithm.5}{}}
\newlabel{Eqn:Arnoldi_decomp}{{6.3.4}{86}{The $m$-Step Arnoldi Iteration}{equation.6.3.4}{}}
\newlabel{Eqn:Arnoldi_decomp_Ritz_pairs}{{6.3.5}{86}{The $m$-Step Arnoldi Iteration}{equation.6.3.5}{}}
\citation{golub2012matrix}
\newlabel{Def:Chebyshev_polys}{{6.3.6}{87}{The $m$-Step Arnoldi Iteration}{equation.6.3.6}{}}
\newlabel{Thm:Lanczos_conv_rate}{{6.3.2}{87}{}{theorem.6.3.2}{}}
\newlabel{Eqn:Lanczos_thm_1}{{6.3.7}{87}{}{equation.6.3.7}{}}
\newlabel{Eqn:Lanczos_thm_2}{{6.3.8}{87}{}{equation.6.3.8}{}}
\newlabel{Cor:Lanczos_cor_for_power_method}{{6.3.3}{87}{}{theorem.6.3.3}{}}
\newlabel{Eqn:Lanczos_cor_for_power_method}{{6.3.9}{87}{}{equation.6.3.9}{}}
\citation{golub2012matrix}
\citation{golub2012matrix}
\newlabel{Eqn:power_method_lower_bound}{{6.3.10}{88}{The $m$-Step Arnoldi Iteration}{equation.6.3.10}{}}
\newlabel{Eqn:Lanczos_lower_bound}{{6.3.11}{88}{The $m$-Step Arnoldi Iteration}{equation.6.3.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Lower bound terms (\ref  {Eqn:power_method_lower_bound}) and (\ref  {Eqn:Lanczos_lower_bound}) for Ritz values generated by Algorithms \ref  {Alg:power_method} and \ref  {Alg:Arnoldi_iteration} \relax }}{88}{table.caption.24}}
\newlabel{Tab:Lanczos_vs_power_method}{{6.2}{88}{Lower bound terms (\ref {Eqn:power_method_lower_bound}) and (\ref {Eqn:Lanczos_lower_bound}) for Ritz values generated by Algorithms \ref {Alg:power_method} and \ref {Alg:Arnoldi_iteration} \relax }{table.caption.24}{}}
\citation{sorensen1992implicit}
\citation{golub2012matrix}
\citation{golub2012matrix}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.3.3}{The $p$-Step Shifted QR Iteration}}{89}{subsection.6.3.3}}
\newlabel{Subsubsec:evol_mats-QR_iteration}{{6.3.3}{89}{The $p$-Step Shifted QR Iteration}{subsection.6.3.3}{}}
\newlabel{Eqn:filter_poly}{{6.3.12}{89}{The $p$-Step Shifted QR Iteration}{equation.6.3.12}{}}
\citation{golub2012matrix}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces $p$-step shifted QR iteration (implicit polynomial filtering)\relax }}{90}{algorithm.6}}
\newlabel{Alg:shifted_QR_iteration}{{6}{90}{$p$-step shifted QR iteration (implicit polynomial filtering)\relax }{algorithm.6}{}}
\newlabel{Prop:Arnoldi_restart}{{6.3.1}{90}{}{thm.6.3.1}{}}
\newlabel{Eqn:Arnoldi_decomp_transf_shifted_QR}{{6.3.13}{90}{The $p$-Step Shifted QR Iteration}{equation.6.3.13}{}}
\citation{golub2012matrix}
\newlabel{Eqn:Arnoldi_decomp_transf_shifted_QR-part1}{{6.3.14}{91}{The $p$-Step Shifted QR Iteration}{equation.6.3.14}{}}
\newlabel{Eqn:Arnoldi_decomp_transf_shifted_QR-part2}{{6.3.15}{91}{The $p$-Step Shifted QR Iteration}{equation.6.3.15}{}}
\newlabel{Eqn:Arnoldi_decomp_j-step_update}{{6.3.16}{91}{The $p$-Step Shifted QR Iteration}{equation.6.3.16}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.3.4}{The Implicitly Restarted Arnoldi Method}}{91}{subsection.6.3.4}}
\newlabel{Subsubsec:evol_mats-IRAM}{{6.3.4}{91}{The Implicitly Restarted Arnoldi Method}{subsection.6.3.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Implicitly restarted Arnoldi method (IRAM)\relax }}{92}{algorithm.7}}
\newlabel{Alg:IRAM}{{7}{92}{Implicitly restarted Arnoldi method (IRAM)\relax }{algorithm.7}{}}
\citation{lehoucq1998arpack}
\@writefile{toc}{\contentsline {section}{\tocsection {}{6.4}{A New Strategy for the EMEP}}{93}{section.6.4}}
\newlabel{Subsec:evol_mats-adaptive_IRAM}{{6.4}{93}{A New Strategy for the EMEP}{section.6.4}{}}
\newlabel{Def:emp_opt}{{6.4.1}{93}{}{thm.6.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces  Performance results for an EMEP from a PLGD model with Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}) with noise ratio $\epsilon _\text  {rel} = 0.15$, oversampling rate $L = 5$, and original signal from Figure \ref  {Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels. Top: Number of matrix-vector products (capped at 1,500 for better viewing) for various EMEP iterates and number of requested eigenvalues $r$. Arnoldi decomposition size is set to $m = 40$ and black dots indicate the empirically optimal values $\mathaccentV {bar}016{r}$. Bottom: Plot of IRAM results for the EMEP with empirically optimal values $\mathaccentV {bar}016{r}$ from top plot and fixed parameters $r=2$ and $m=20$. \relax }}{94}{figure.caption.25}}
\newlabel{Fig:Numerics-num_matvecs_orig_vs_optimal_params}{{6.5}{94}{Performance results for an EMEP from a PLGD model with Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}) with noise ratio $\epsilon _\text {rel} = 0.15$, oversampling rate $L = 5$, and original signal from Figure \ref {Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels. Top: Number of matrix-vector products (capped at 1,500 for better viewing) for various EMEP iterates and number of requested eigenvalues $r$. Arnoldi decomposition size is set to $m = 40$ and black dots indicate the empirically optimal values $\bar {r}$. Bottom: Plot of IRAM results for the EMEP with empirically optimal values $\bar {r}$ from top plot and fixed parameters $r=2$ and $m=20$. \relax }{figure.caption.25}{}}
\newlabel{Eqn:adaptive_delta_2}{{6.4.1}{95}{A New Strategy for the EMEP}{equation.6.4.1}{}}
\newlabel{Eqn:adaptive_delta_4_lin_interp_prob}{{6.4.2}{96}{A New Strategy for the EMEP}{equation.6.4.2}{}}
\newlabel{Eqn:adaptive_delta_4}{{6.4.3}{96}{A New Strategy for the EMEP}{equation.6.4.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces The IRAM with adaptive parameter selection for the EMEP (\ref  {Eqn:EMEP_PLGD})\relax }}{97}{algorithm.8}}
\newlabel{Alg:adaptive_IRAM}{{8}{97}{The IRAM with adaptive parameter selection for the \emep \relax }{algorithm.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces  Plot comparing Algorithm \ref  {Alg:adaptive_IRAM} with the empirically optimal value $\mathaccentV {bar}016{r}$. The EMEP (\ref  {Eqn:EMEP_PLGD})\ is from Figure \ref  {Fig:Numerics-num_matvecs_orig_vs_optimal_params} and Arnoldi decomposition (\ref  {Eqn:Arnoldi_decomp}) size is $m=40$. \relax }}{98}{figure.caption.26}}
\newlabel{Fig:Numerics-num_eigs_ada_vs_opt_one_plot}{{6.6}{98}{Plot comparing Algorithm \ref {Alg:adaptive_IRAM} with the empirically optimal value $\bar {r}$. The \emep \ is from Figure \ref {Fig:Numerics-num_matvecs_orig_vs_optimal_params} and Arnoldi decomposition (\ref {Eqn:Arnoldi_decomp}) size is $m=40$. \relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{6.5}{Spectrum Distribution and IRAM Behavior}}{99}{section.6.5}}
\newlabel{Subsec:evol_mats-correl_btwn_EMEP_and_IRAM}{{6.5}{99}{Spectrum Distribution and IRAM Behavior}{section.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces  Number of requested eigenvalues $r$ chosen by Algorithm \ref  {Alg:adaptive_IRAM} for the EMEP (\ref  {Eqn:EMEP_PLGD})\ from two PLGD models with Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}), with noise ratio $\epsilon _\text  {rel}=0.15, 0.30$, oversampling rate $L=5$, and original signal from Figure \ref  {Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels. \relax }}{99}{figure.caption.27}}
\newlabel{Fig:Numerics-num_req_eigs_2_exps}{{6.7}{99}{Number of requested eigenvalues $r$ chosen by Algorithm \ref {Alg:adaptive_IRAM} for the \emep \ from two PLGD models with Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}), with noise ratio $\epsilon _\text {rel}=0.15, 0.30$, oversampling rate $L=5$, and original signal from Figure \ref {Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels. \relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.5.1}{Eigenvalue Clustering and IRAM Parameter Choice}}{100}{subsection.6.5.1}}
\newlabel{Subsubsec:evol_mats-correl_clustering_IRAM_param}{{6.5.1}{100}{Eigenvalue Clustering and IRAM Parameter Choice}{subsection.6.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Behavior of Algorithm \ref  {Alg:adaptive_IRAM} for the experiment from Figure \ref  {Fig:Numerics-num_req_eigs_2_exps} with $L=5$, $\epsilon _\text  {rel}=0.15$. Top: Number of matrix-vector products (capped at 2,000 for better viewing) for each EMEP (\ref  {Eqn:EMEP_PLGD})\ iterate $k$ and number of requested eigenvalues $r$. Bottom: eigenvalue differences $\lambda _r - \lambda _{r+1}$ for each EMEP (\ref  {Eqn:EMEP_PLGD})\ iterate $k$ and number of requested eigenvalues $r$. Black dots in both plots indicate the value $r_k$ chosen by Algorithm \ref  {Alg:adaptive_IRAM}.\relax }}{101}{figure.caption.28}}
\newlabel{Fig:Numerics-surf_mvs_eig_diffs_1}{{6.8}{101}{Behavior of Algorithm \ref {Alg:adaptive_IRAM} for the experiment from Figure \ref {Fig:Numerics-num_req_eigs_2_exps} with $L=5$, $\epsilon _\text {rel}=0.15$. Top: Number of matrix-vector products (capped at 2,000 for better viewing) for each \emep \ iterate $k$ and number of requested eigenvalues $r$. Bottom: eigenvalue differences $\lambda _r - \lambda _{r+1}$ for each \emep \ iterate $k$ and number of requested eigenvalues $r$. Black dots in both plots indicate the value $r_k$ chosen by Algorithm \ref {Alg:adaptive_IRAM}.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Behavior of Algorithm \ref  {Alg:adaptive_IRAM} for the experiment from Figure \ref  {Fig:Numerics-num_req_eigs_2_exps} with $L=5$, $\epsilon _\text  {rel}=0.30$. Top: Number of matrix-vector products (capped at 2,000 for better viewing) for each EMEP (\ref  {Eqn:EMEP_PLGD})\ iterate $k$ and number of requested eigenvalues $r$. Bottom: eigenvalue differences $\lambda _r - \lambda _{r+1}$ for each EMEP (\ref  {Eqn:EMEP_PLGD})\ iterate $k$ and number of requested eigenvalues $r$. Black dots in both plots indicate the value $r_k$ chosen by Algorithm \ref  {Alg:adaptive_IRAM}.\relax }}{102}{figure.caption.29}}
\newlabel{Fig:Numerics-surf_mvs_eig_diffs_2}{{6.9}{102}{Behavior of Algorithm \ref {Alg:adaptive_IRAM} for the experiment from Figure \ref {Fig:Numerics-num_req_eigs_2_exps} with $L=5$, $\epsilon _\text {rel}=0.30$. Top: Number of matrix-vector products (capped at 2,000 for better viewing) for each \emep \ iterate $k$ and number of requested eigenvalues $r$. Bottom: eigenvalue differences $\lambda _r - \lambda _{r+1}$ for each \emep \ iterate $k$ and number of requested eigenvalues $r$. Black dots in both plots indicate the value $r_k$ chosen by Algorithm \ref {Alg:adaptive_IRAM}.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.5.2}{Selecting the Arnoldi Decomposition Size}}{104}{subsection.6.5.2}}
\newlabel{Subsubsec:evol_mats-default_Arnoldi_decomp_size}{{6.5.2}{104}{Selecting the Arnoldi Decomposition Size}{subsection.6.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces  Number of matrix-vector products (capped at 3,000 for better viewing) for individual EMEP (\ref  {Eqn:EMEP_PLGD})\ iterates with varying number of requested eigenvalues $r$ and Arnoldi decomposition (\ref  {Eqn:Arnoldi_decomp}) size $m$. Left: EMEP iterate 180 from Figure \ref  {Fig:Numerics-surf_mvs_eig_diffs_1}. Right: EMEP iterate 85 from Figure \ref  {Fig:Numerics-surf_mvs_eig_diffs_2}. \relax }}{104}{figure.caption.30}}
\newlabel{Fig:Numerics-surf_mvs_for_m_vs_j}{{6.10}{104}{Number of matrix-vector products (capped at 3,000 for better viewing) for individual \emep \ iterates with varying number of requested eigenvalues $r$ and Arnoldi decomposition (\ref {Eqn:Arnoldi_decomp}) size $m$. Left: EMEP iterate 180 from Figure \ref {Fig:Numerics-surf_mvs_eig_diffs_1}. Right: EMEP iterate 85 from Figure \ref {Fig:Numerics-surf_mvs_eig_diffs_2}. \relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {chapter}{\tocchapter {Chapter}{7}{The Improved Gauge Dual Descent Algorithm \\and Performance Results}}{106}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:Numerics}{{7}{106}{The Improved Gauge Dual Descent Algorithm \\and Performance Results}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{7.1}{Introduction}}{106}{section.7.1}}
\newlabel{Subsec:Numerics-intro}{{7.1}{106}{Introduction}{section.7.1}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{7.2}{The Improved Gauge Dual Descent Algorithm}}{106}{section.7.2}}
\newlabel{Subsec:Numerics-improved_PGD}{{7.2}{106}{The Improved Gauge Dual Descent Algorithm}{section.7.2}{}}
\newlabel{Eqn:term_crit_new-primal_difference2}{{7.2.1}{107}{The Improved Gauge Dual Descent Algorithm}{equation.7.2.1}{}}
\newlabel{Eqn:term_crit_new-dual_difference2}{{7.2.2}{107}{The Improved Gauge Dual Descent Algorithm}{equation.7.2.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces Improved Gauge Dual Descent (IGDD) algorithm\relax }}{108}{algorithm.9}}
\newlabel{Alg:PGD-improved}{{9}{108}{Improved Gauge Dual Descent (IGDD) algorithm\relax }{algorithm.9}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{7.3}{Performance Results}}{109}{section.7.3}}
\newlabel{Subsec:Numerics-perf_results}{{7.3}{109}{Performance Results}{section.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{7.3.1}{Random Gaussian Signals and Large-Scale Images}}{109}{subsection.7.3.1}}
\newlabel{Subsec:Numerics-perf_results_random}{{7.3.1}{109}{Random Gaussian Signals and Large-Scale Images}{subsection.7.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces  Performance results for PLGD models with Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}), where signals are complex with standard Gaussian distribution (\ref  {Def:Gaussian_distribution_complex}). Results indicate the GDD algorithm (solid line) and IGDD algorithm (dashed line). Each result is the mean of 10 experiments. Left: Varying signal size $n$, with fixed noise ratio $\epsilon _\text  {rel} = 0.15$ and oversampling scaled logarithmically with $n$ (i.e., $L = 10, 12, 12, 14$) as indicated in Theorem \ref  {Thm:PhaseLift_approx}. Middle: Varying oversampling rate $L$, with fixed signal size $n = 128$ and noise ratio $\epsilon _\text  {rel} = 0.15$. Right: Varying noise ratio $\epsilon _\text  {rel}$, with fixed signal size $n = 128$ and oversampling rate $L = 10$. \relax }}{109}{figure.caption.31}}
\newlabel{Fig:Numerics-ada_vs_orig_various_params}{{7.1}{109}{Performance results for PLGD models with Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}), where signals are complex with standard Gaussian distribution (\ref {Def:Gaussian_distribution_complex}). Results indicate the GDD algorithm (solid line) and IGDD algorithm (dashed line). Each result is the mean of 10 experiments. Left: Varying signal size $n$, with fixed noise ratio $\epsilon _\text {rel} = 0.15$ and oversampling scaled logarithmically with $n$ (i.e., $L = 10, 12, 12, 14$) as indicated in Theorem \ref {Thm:PhaseLift_approx}. Middle: Varying oversampling rate $L$, with fixed signal size $n = 128$ and noise ratio $\epsilon _\text {rel} = 0.15$. Right: Varying noise ratio $\epsilon _\text {rel}$, with fixed signal size $n = 128$ and oversampling rate $L = 10$. \relax }{figure.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces  Performance results for PLGD models with Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}), where signals are the images in Figure \ref  {Fig:Numerics-large_images}. Results indicate total number of matrix-vector products and percent decrease from the GDD algorithm. Parameter $m$ is the Arnoldi decomposition size (\ref  {Eqn:Arnoldi_decomp}) for the IRAM (Algorithm \ref  {Alg:IRAM}). \relax }}{110}{table.caption.32}}
\newlabel{Tab:Numerics-ada_vs_orig_large_images}{{7.1}{110}{Performance results for PLGD models with Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}), where signals are the images in Figure \ref {Fig:Numerics-large_images}. Results indicate total number of matrix-vector products and percent decrease from the GDD algorithm. Parameter $m$ is the Arnoldi decomposition size (\ref {Eqn:Arnoldi_decomp}) for the IRAM (Algorithm \ref {Alg:IRAM}). \relax }{table.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces  Images used for experiments in Table \ref  {Tab:Numerics-ada_vs_orig_large_images}. Top left: original image of my daughter and me, image size $240 \times 240 = 57,600$ pixels. Bottom left: result image after solving EMEP. Top right: original image of UC Davis roundabout, image size $200 \times 600 = 120,000$ pixels. Bottom right: result image after solving EMEP. All experiments have noise ratio $\epsilon _\text  {rel} = 0.15$ and oversampling $L = 15$. \relax }}{111}{figure.caption.33}}
\newlabel{Fig:Numerics-large_images}{{7.2}{111}{Images used for experiments in Table \ref {Tab:Numerics-ada_vs_orig_large_images}. Top left: original image of my daughter and me, image size $240 \times 240 = 57,600$ pixels. Bottom left: result image after solving EMEP. Top right: original image of UC Davis roundabout, image size $200 \times 600 = 120,000$ pixels. Bottom right: result image after solving EMEP. All experiments have noise ratio $\epsilon _\text {rel} = 0.15$ and oversampling $L = 15$. \relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{7.3.2}{IGDD vs Empirically Optimal Parameter Selection}}{111}{subsection.7.3.2}}
\newlabel{Subsec:Numerics-perf_results_emp_optimal}{{7.3.2}{111}{IGDD vs Empirically Optimal Parameter Selection}{subsection.7.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces  Performance results for PLGD models with Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}) with original signal from Figure \ref  {Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels. Results indicate total number of matrix-vector products and percent decrease from the GDD algorithm. Parameter $m$ is the Arnoldi decomposition size (\ref  {Eqn:Arnoldi_decomp}). \relax }}{112}{table.caption.34}}
\newlabel{Tab:Numerics-num_matvecs_opt_vs_ada}{{7.2}{112}{Performance results for PLGD models with Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}) with original signal from Figure \ref {Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels. Results indicate total number of matrix-vector products and percent decrease from the GDD algorithm. Parameter $m$ is the Arnoldi decomposition size (\ref {Eqn:Arnoldi_decomp}). \relax }{table.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces  Number of requested eigenvalues $r$ in the IRAM (Algorithm \ref  {Alg:IRAM}) for two PLGD models from Table \ref  {Tab:Numerics-num_matvecs_opt_vs_ada}. \relax }}{113}{figure.caption.35}}
\newlabel{Fig:Numerics-num_eigs_ada_vs_opt}{{7.3}{113}{Number of requested eigenvalues $r$ in the IRAM (Algorithm \ref {Alg:IRAM}) for two PLGD models from Table \ref {Tab:Numerics-num_matvecs_opt_vs_ada}. \relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{7.3.3}{Selection of Arnoldi Decomposition Parameter}}{114}{subsection.7.3.3}}
\newlabel{Subsec:Numerics-perf_results_emp_arnoldi_param}{{7.3.3}{114}{Selection of Arnoldi Decomposition Parameter}{subsection.7.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces  Total number of matrix-vector products for various PLGD models with Gaussian noise (\ref  {Eqn:PhaseLift-GD_Gaussian_noise}) with original signal from Figure \ref  {Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels. Parameter $r$ is the number of requested eigenvalues in the IRAM (Algorithm \ref  {Alg:IRAM}) and $m$ is the Arnoldi decomposition (\ref  {Eqn:Arnoldi_decomp}) size. \relax }}{114}{table.caption.36}}
\newlabel{Tab:Numerics-num_matvecs_orig_vs_ada}{{7.3}{114}{Total number of matrix-vector products for various PLGD models with Gaussian noise (\ref {Eqn:PhaseLift-GD_Gaussian_noise}) with original signal from Figure \ref {Fig:parrot_signal_iterates} resized to $64 \times 64$ pixels. Parameter $r$ is the number of requested eigenvalues in the IRAM (Algorithm \ref {Alg:IRAM}) and $m$ is the Arnoldi decomposition (\ref {Eqn:Arnoldi_decomp}) size. \relax }{table.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Number of matrix-vector products for each EMEP (\ref  {Eqn:EMEP_PLGD})\ iteration from two experiments in Figure \ref  {Fig:Numerics-num_matvecs_ada_for_m_vals} with various Arnoldi decomposition size $m=20, 40, 80$.\relax }}{115}{figure.caption.37}}
\newlabel{Fig:Numerics-num_matvecs_ada_for_m_vals}{{7.4}{115}{Number of matrix-vector products for each \emep \ iteration from two experiments in Figure \ref {Fig:Numerics-num_matvecs_ada_for_m_vals} with various Arnoldi decomposition size $m=20, 40, 80$.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {chapter}{\tocchapter {Chapter}{8}{Conclusion}}{116}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:Conclusion}{{8}{116}{Conclusion}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{8.1}{Contributions}}{116}{section.8.1}}
\newlabel{Subsec:Conclusion-contrib}{{8.1}{116}{Contributions}{section.8.1}{}}
\citation{golub2002inverse}
\citation{schmidt2005minFunc}
\citation{sun2016geometric}
\citation{schmidt2008minConf}
\@writefile{toc}{\contentsline {section}{\tocsection {}{8.2}{Future Work}}{117}{section.8.2}}
\newlabel{Subsec:Conclusion-future}{{8.2}{117}{Future Work}{section.8.2}{}}
\@writefile{toc}{\contentsline {chapter}{\tocappendix {Appendix}{A}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}}{119}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:Appx-Comparison}{{A}{119}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}{appendix.A}{}}
\citation{boyd2004convex}
\citation{candes2013phaselift}
\citation{becker2011templates}
\citation{rockafellar1970convex}
\citation{candes2013phaselift}
\citation{DBLP:journals/siamsc/FriedlanderM16}
\newlabel{Eqn:PLGD_ch2}{{A.0.1}{120}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}{equation.A.0.1}{}}
\newlabel{Eqn:PhaseLift-Lagrange_dual_nonTFOCS}{{A.0.2}{120}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}{equation.A.0.2}{}}
\newlabel{Eqn:PhaseLift_Lagrange_dual}{{A.0.3}{120}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}{equation.A.0.3}{}}
\citation{candes2014solving}
\citation{boyd2004convex}
\newlabel{Eqn:PhaseLift_l-1_norm_model}{{A.0.4}{121}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}{equation.A.0.4}{}}
\newlabel{Eqn:PhaseLift_l-1_norm_model_dual}{{A.0.5}{121}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}{equation.A.0.5}{}}
\newlabel{Eqn:PhaseLift_l-1_norm_model_general}{{A.0.6}{121}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}{equation.A.0.6}{}}
\newlabel{Eqn:signal_recovery_inequality}{{A.0.9}{122}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}{equation.A.0.9}{}}
\newlabel{Eqn:angle_eta_y}{{A.0.10}{123}{A Comparison of PhaseLift-type Models \\and Phase Retrieval Algorithms}{equation.A.0.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Rate of successful signal recovery and mean residual values for sets of 100 noisy phase retrieval problems with random Gaussian signals of size $n = 128$ with oversampling rate $L$ and relative error $\epsilon _\textnormal  {rel}$. The term \textit  {xErr} is signal relative error $||xx^*- \mathbf  {x}\mathbf  {x}^*||_F / ||\mathbf  {x}\mathbf  {x}^*||_F$. Recovery is determined successful if the inequality (\ref  {Eqn:signal_recovery_inequality}) is satisfied for a given $\tau $. The GDD algorithm (Algorithm \ref  {Alg:PGD}) is set to terminate at 100 iterations. Numbers $n_{-k}$ are shorthand for $n \times 10^{-k}$.\relax }}{123}{table.caption.38}}
\newlabel{Tab:relative_errors_saga_vs_wflow}{{A.1}{123}{Rate of successful signal recovery and mean residual values for sets of 100 noisy phase retrieval problems with random Gaussian signals of size $n = 128$ with oversampling rate $L$ and relative error $\epsilon _\rel $. The term \textit {xErr} is signal relative error $||xx^*- \mathbf {x}\mathbf {x}^*||_F / ||\mathbf {x}\mathbf {x}^*||_F$. Recovery is determined successful if the inequality (\ref {Eqn:signal_recovery_inequality}) is satisfied for a given $\tau $. The GDD algorithm (Algorithm \ref {Alg:PGD}) is set to terminate at 100 iterations. Numbers $n_{-k}$ are shorthand for $n \times 10^{-k}$.\relax }{table.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Results from the GDD algorithm (Algorithm \ref  {Alg:PGD}) applied to a test image separated into its three RGB channels. Left: signal relative error $||xx^*- \mathbf  {x}\mathbf  {x}^*||_F / ||\mathbf  {x}\mathbf  {x}^*||_F$. Right: primal relative error $||{\mathcal  A}(xx^*)-b||_2 / ||b||_2$ with red line indicating primal feasibility. Red circles denote the iterate signals pictured above. Original signal is $128 \times 128$ pixels, with an oversampling of $L = 8$ and noise ratio $\epsilon _\textnormal  {rel}= 0.30$. Measurements use the mean of the three color channel values.\relax }}{124}{figure.caption.39}}
\newlabel{Fig:parrot_signal_relative_error_2}{{A.1}{124}{Results from the GDD algorithm (Algorithm \ref {Alg:PGD}) applied to a test image separated into its three RGB channels. Left: signal relative error $||xx^*- \mathbf {x}\mathbf {x}^*||_F / ||\mathbf {x}\mathbf {x}^*||_F$. Right: primal relative error $||\caA (xx^*)-b||_2 / ||b||_2$ with red line indicating primal feasibility. Red circles denote the iterate signals pictured above. Original signal is $128 \times 128$ pixels, with an oversampling of $L = 8$ and noise ratio $\epsilon _\rel = 0.30$. Measurements use the mean of the three color channel values.\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {chapter}{\tocappendix {Appendix}{B}{Further Justification of New Termination Conditions in Section\nonbreakingspace \ref  {Subsec:PLGD_term_crit-new_term_crit}}}{125}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Sec:Appx-further_reasons_for_new_term_crit}{{B}{125}{Further Justification of New Termination Conditions in Section~\ref {Subsec:PLGD_term_crit-new_term_crit}}{appendix.B}{}}
\newlabel{Eqn:GD_new_termination_conds-primal_feas}{{B.0.1}{125}{Further Justification of New Termination Conditions in Section~\ref {Subsec:PLGD_term_crit-new_term_crit}}{equation.B.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Primal relative error (\ref  {Eqn:term_crit-candidate_residuals}d) (blue) and noise ratio threshold (red) for noise ratio $\epsilon _\textnormal  {rel}= 0.15$ and oversampling rate $L = 5$. Iterate 1,000 remains infeasible with a primal relative error of $0.1541$.\relax }}{126}{figure.caption.40}}
\newlabel{Fig:term_crit-pr_err_fails}{{B.1}{126}{Primal relative error (\ref {Eqn:term_crit-candidate_residuals}d) (blue) and noise ratio threshold (red) for noise ratio $\epsilon _\rel = 0.15$ and oversampling rate $L = 5$. Iterate 1,000 remains infeasible with a primal relative error of $0.1541$.\relax }{figure.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Final values of duality gap (\ref  {Eqn:term_crit-candidate_residuals}f) after 1,000 iterations of the GDD algorithm (Algorithm \ref  {Alg:PGD}) with indicated noise ratios and oversampling rates.\relax }}{126}{table.caption.41}}
\newlabel{Tab:term_crit-duality_gap_stagnates}{{B.1}{126}{Final values of duality gap (\ref {Eqn:term_crit-candidate_residuals}f) after 1,000 iterations of the GDD algorithm (Algorithm \ref {Alg:PGD}) with indicated noise ratios and oversampling rates.\relax }{table.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Plots of dual objective difference values (\ref  {Eqn:term_crit-candidate_residuals}h) against the iterate at which the GDD algorithm (Algorithm \ref  {Alg:PGD}) first satisfies this tolerance for the models discussed in Figure \ref  {Fig:term_crit-signal_err}. Red circles are placed at tolerances $10^{-n}$. The blue rectangles indicate the proposed intervals of termination from Table \ref  {Tab:term_crit-desired_termination_windows}.\relax }}{128}{figure.caption.42}}
\newlabel{Fig:term_crit-dual_obj}{{B.2}{128}{Plots of dual objective difference values (\ref {Eqn:term_crit-candidate_residuals}h) against the iterate at which the GDD algorithm (Algorithm \ref {Alg:PGD}) first satisfies this tolerance for the models discussed in Figure \ref {Fig:term_crit-signal_err}. Red circles are placed at tolerances $10^{-n}$. The blue rectangles indicate the proposed intervals of termination from Table \ref {Tab:term_crit-desired_termination_windows}.\relax }{figure.caption.42}{}}
\bibstyle{siam}
\bibdata{DissertationBibliography.bib}
\bibcite{aravkin2017foundations}{1}
\bibcite{bahmani2016phase}{2}
\bibcite{bai2000eigenvaluetemplates}{3}
\bibcite{barzilai1988twopoint}{4}
\bibcite{becker2011templates}{5}
\bibcite{ben2001lectures}{6}
\bibcite{bertsekas2016nonlinear}{7}
\bibcite{boyd2004convex}{8}
\bibcite{bracewell1986fourier}{9}
\bibcite{bunk2007diffractive}{10}
\bibcite{cai2016optimal}{11}
\bibcite{DBLP:journals/siamis/CandesESV13}{12}
\bibcite{candes2014solving}{13}
\bibcite{DBLP:journals/tit/CandesLS15}{14}
\bibcite{candes2006stable}{15}
\@writefile{toc}{\contentsline {chapter}{\tocchapter {\chaptername }{}{Bibliography}}{129}{appendix*.43}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{candes2013phaselift}{16}
\bibcite{chai2010array}{17}
\bibcite{chen2001atomic}{18}
\bibcite{comon1990tracking}{19}
\bibcite{doukopoulos2008fast}{20}
\bibcite{duadi2011digital}{21}
\bibcite{DBLP:journals/corr/abs-1211-0872}{22}
\bibcite{elser2017benchmark}{23}
\bibcite{Fienup82}{24}
\bibcite{fienup1987phase}{25}
\bibcite{DBLP:journals/mp/Freund87}{26}
\bibcite{DBLP:journals/siamsc/FriedlanderM16}{27}
\bibcite{DBLP:journals/siamjo/FriedlanderMP14}{28}
\bibcite{GS72}{29}
\bibcite{goldstein2018phasemax}{30}
\bibcite{golub2012matrix}{31}
\bibcite{golub2002inverse}{32}
\bibcite{grigorieff1991note}{33}
\bibcite{Guizar-Sicairos}{34}
\bibcite{harrison1993phase}{35}
\bibcite{DBLP:journals/corr/JaganathanEH15a}{36}
\bibcite{jiang2017robust}{37}
\bibcite{katkovnik2017phase}{38}
\bibcite{lehoucq1998arpack}{39}
\bibcite{LeviS84}{40}
\bibcite{martin2012noise}{41}
\bibcite{miao1999extending}{42}
\bibcite{miao2008extending}{43}
\bibcite{millane1990phase}{44}
\bibcite{natarajan1995sparse}{45}
\bibcite{ngo2012scaled}{46}
\bibcite{nocedal2006numerical}{47}
\bibcite{parikh2014proximal}{48}
\bibcite{recht2010guaranteed}{49}
\bibcite{reed1980functional}{50}
\bibcite{rockafellar1970convex}{51}
\bibcite{rodriguez2013oversampling}{52}
\bibcite{saad2011numerical}{53}
\bibcite{saad2016analysis}{54}
\bibcite{saad2010numerical}{55}
\bibcite{schmidt2005minFunc}{56}
\bibcite{schmidt2008minConf}{57}
\bibcite{schmidt2009optimizing}{58}
\bibcite{shechtman2014gespar}{59}
\bibcite{DBLP:journals/spm/ShechtmanECCMS15}{60}
\bibcite{DBLP:journals/corr/abs-1104-4406}{61}
\bibcite{sorensen1992implicit}{62}
\bibcite{sorensen1997implicitly}{63}
\bibcite{stewart1992updating}{64}
\bibcite{sun2016geometric}{65}
\bibcite{toh1999sdpt3}{66}
\bibcite{walther1963question}{67}
\bibcite{yang1995projection}{68}
\bibcite{zhang2004nonmonotone}{69}
\bibcite{zhang2017fast}{70}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{72.39178pt}
\newlabel{tocindent1}{27.98338pt}
\newlabel{tocindent2}{0pt}
\newlabel{tocindent3}{0pt}
